"""Research executor - OpenRouter or Claude agent with research workflow.

Research artifacts are research questions answered via web search.
Uses research_workflow workflow for automatic tool loop and forced output (OpenRouter),
or Claude agent with SDK native output_format for structured output.

Supports two backends:
- OpenRouter (default): Uses research_workflow with tool loop
- Claude agent: Uses Agent with expected_files_struct_out_field for file validation

Uses aii_lib for:
- OpenRouterClient: LLM calls (OpenRouter backend)
- research_workflow: Research workflow with tool loop + force output
- Agent/AgentOptions: Claude agent calls with file validation
- AIITelemetry: Task tracking

File validation (Claude agent path) uses structured output (expected_files_struct_out_field on AgentOptions):
- Agent reports all created file paths in typed expected_files structure
- SDK recursively extracts paths and validates they exist inside workspace
- Automatically retries with feedback on missing files
"""

from __future__ import annotations

import asyncio
import json
from pathlib import Path

from aii_lib import (
    OpenRouterClient,
    AgentInitializer,
    AgentFinalizer,
    AIITelemetry,
    MessageType,
    research_workflow,
    ResearchWorkflowConfig,
)
from aii_lib.agent_backend import Agent, AgentOptions, aggregate_summaries
from aii_lib.abilities.mcp_server.config import get_tooluniverse_mcp_config

from aii_pipeline.prompts.steps._3_invention_loop._3_gen_art.research.u_prompt import (
    get as get_exec_prompt,
    get_force_output_prompt,
    build_research_retry_prompt,
)
from aii_pipeline.prompts.steps._3_invention_loop._3_gen_art.research.s_prompt import get as get_exec_sysprompt
from aii_pipeline.prompts.steps._3_invention_loop._3_gen_art.research.schema import (
    ResearchArtifact,
    verify_research_output,
)
from aii_pipeline.utils import PipelineConfig
from aii_pipeline.prompts.steps._3_invention_loop._2_gen_plan.schema import BasePlan

from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from aii_pipeline.steps._invention_loop.pools import ArtifactPool


def _write_research_report(research_result: dict, workspace_dir: Path) -> None:
    """Write research_report.md to workspace from result dict."""
    title = research_result.get("title", "Research")
    summary = research_result.get("summary", "")
    answer = research_result.get("answer", "")
    sources = research_result.get("sources", [])
    follow_up = research_result.get("follow_up_questions", [])

    md = f"# {title}\n\n"
    if summary:
        md += f"## Summary\n\n{summary}\n\n"
    if answer:
        md += f"## Research Findings\n\n{answer}\n\n"
    if sources:
        md += "## Sources\n\n"
        for src in sources:
            idx = src.get("index", "")
            url = src.get("url", "")
            src_title = src.get("title", "")
            src_summary = src.get("summary", "")
            prefix = f"[{idx}] "
            if url:
                md += f"{prefix}[{src_title}]({url})"
            else:
                md += f"{prefix}{src_title}"
            if src_summary:
                md += f" — {src_summary}"
            md += "\n\n"
    if follow_up:
        md += "## Follow-up Questions\n\n"
        for q in follow_up:
            md += f"- {q}\n"
        md += "\n"
    md += "---\n*Generated by AI Inventor Pipeline*\n"

    report_path = workspace_dir / "research_report.md"
    report_path.write_text(md, encoding="utf-8")


# Tools available for research
RESEARCH_TOOLS = ["aii_web_search_fast", "aii_web_fetch_direct", "aii_web_fetch_grep"]


async def _execute_research_claude_agent(
    plan: BasePlan,
    artifact_pool: ArtifactPool,
    config: PipelineConfig,
    run_dir: Path,
    workspace_dir: Path,
    iteration: int,
    research_idx: int,
    finalizer: AgentFinalizer,
    telemetry: AIITelemetry | None,
    task_id: str | None,
    task_name: str | None,
) -> tuple[dict, bool, float]:
    """Execute a research plan using Claude agent with structured output."""
    research_cfg = config.invention_loop.execute.research
    claude_cfg = research_cfg.claude_agent

    # Get verification config
    verify_retries = research_cfg.verify_retries

    # Build plan text from typed fields
    plan_text = plan.to_prompt_yaml()

    # Build prompt - agent_mode=True for absolute workspace paths (read-only)
    prompt = get_exec_prompt(
        plan_text=plan_text,
        artifact_pool=artifact_pool,
        dependency_ids=plan.artifact_dependencies,
        agent_mode=True,
        workspace_path=str(workspace_dir),
    )
    system_prompt = get_exec_sysprompt()

    # Use absolute path for output file
    abs_cwd = workspace_dir.resolve()
    output_file = str(abs_cwd / "research_out.json")

    # Create callback for group tracking (summary aggregation)
    callback = None
    if telemetry and task_id and task_name:
        callback = telemetry.create_callback(task_id, task_name, group="research")

    try:
        options = AgentOptions(
            model=claude_cfg.model,
            cwd=workspace_dir,
            max_turns=claude_cfg.max_turns,
            permission_mode="bypassPermissions",
            system_prompt=system_prompt,
            # Telemetry integration
            telemetry=telemetry,
            run_id=task_id,
            agent_context=task_id,
            # Agent-level timeouts/retries
            agent_timeout=claude_cfg.agent_timeout,
            agent_retries=claude_cfg.agent_retries,
            seq_prompt_timeout=claude_cfg.seq_prompt_timeout,
            seq_prompt_retries=claude_cfg.seq_prompt_retries,
            message_timeout=claude_cfg.message_timeout,
            message_retries=claude_cfg.message_retries,
            # SDK native structured output
            output_format=ResearchArtifact.to_struct_output(),
            # Expected files validation via structured output (auto-retry on missing)
            expected_files_struct_out_field="out_expected_files",
            max_expected_files_retries=verify_retries,
            # Always connect MCP for aii_web_fetch_grep.
            # When use_aii_web_tools=True: all 3 MCP tools replace built-in WebSearch/WebFetch.
            # When use_aii_web_tools=False: only grep via MCP, built-in WebSearch/WebFetch for the rest.
            mcp_servers=get_tooluniverse_mcp_config(use_aii_server=True),
            disallowed_tools=(
                ["WebSearch", "WebFetch"] if claude_cfg.use_aii_web_tools
                else ["mcp__aii_tooluniverse__aii_web_search_fast", "mcp__aii_tooluniverse__aii_web_fetch_direct"]
            ) + ["mcp__aii_tooluniverse__dblp_bib_search", "mcp__aii_tooluniverse__dblp_bib_fetch"],
        )

        agent = Agent(options)
        all_responses: list = []  # Track for cost aggregation

        # SDK handles file existence validation + retry automatically
        response = await agent.run(prompt)
        all_responses.append(response)
        cost = response.total_cost

        # Emit ONE aggregated summary (not individual prompt summaries)
        if callback and response.prompt_results:
            aggregated = aggregate_summaries(response.prompt_results)
            if aggregated:
                callback(aggregated)

        if response.failed:
            err = response.error_message or "unknown error"
            finalizer.end_task_failure(f"Agent failed: {err}", cost=cost)
            return {"error": f"Agent failed: {err}"}, False, cost


        # =================================================================
        # POST-VALIDATION: Schema check (with retry)
        # =================================================================
        schema_max_retries = research_cfg.schema_retries

        for schema_attempt in range(1, schema_max_retries + 2):
            verification = verify_research_output(
                workspace_dir=workspace_dir,
                expected_files=ResearchArtifact.get_expected_out_files(),
            )

            if verification.get("schema_errors"):
                for err in verification["schema_errors"][:3]:
                    if telemetry and task_id and task_name:
                        telemetry.emit_message("WARNING", f"Schema: {err}", task_name, task_id)

            if verification.get("valid", False) or schema_attempt > schema_max_retries:
                break

            retry_prompt = build_research_retry_prompt(
                verification=verification,
                attempt=schema_attempt,
                max_attempts=schema_max_retries,
            )
            if telemetry and task_id and task_name:
                telemetry.emit_message(
                    "RETRY",
                    f"Schema validation failed, retrying ({schema_attempt}/{schema_max_retries})...",
                    task_name, task_id,
                )
            retry_result = await agent.run(retry_prompt)
            all_responses.append(retry_result)
            cost += retry_result.total_cost

        # =================================================================
        # COLLECT RESULTS
        # =================================================================
        if response.structured_output:
            data = response.structured_output if isinstance(response.structured_output, dict) else response.structured_output

            answer = data.get("answer", "")
            if not answer:
                finalizer.end_task_failure("No answer in output", cost=cost)
                return {"error": "No answer in output"}, False, cost

            title = data.get("title", "") or plan.title.strip()
            sources = data.get("sources", [])
            follow_up_questions = data.get("follow_up_questions", [])
            summary = data.get("summary", "")

            if not summary:
                summary = f"Research: {answer[:180]}..." if len(answer) > 180 else f"Research: {answer}"

            # Build result
            research_result = {
                "question": plan.question or plan.research_plan,
                "answer": answer,
                "title": title,
                "sources": sources,
                "follow_up_questions": follow_up_questions,
                "summary": summary,
                "model": claude_cfg.model,
                "tool_calls": {},  # Agent doesn't track tool calls the same way
                "iterations_used": 0,
                "forced_output": False,
                "workspace_path": str(workspace_dir),
            }

            # Write research_report.md (demo file)
            _write_research_report(research_result, workspace_dir)

            if telemetry and task_id and task_name:
                schema_ok = verification.get("valid", False)
                telemetry.emit_message(
                    "SUCCESS" if schema_ok else "WARNING",
                    f"Research {'complete' if schema_ok else 'complete (schema issues)'}: {len(answer)} chars, {len(sources)} sources, ${cost:.4f}",
                    task_name, task_id,
                )
            finalizer.end_task_success(cost=cost)
            return research_result, True, cost

        finalizer.end_task_failure("No output from agent", cost=cost)
        raise RuntimeError("No output from Claude agent for research executor")

    except Exception as e:
        finalizer.end_task_error(str(e))
        raise


async def execute_research(
    plan: BasePlan,
    artifact_pool: ArtifactPool,
    config: PipelineConfig,
    run_dir: Path,
    iteration: int = 1,
    research_idx: int = 0,
    telemetry: AIITelemetry | None = None,
    task_id: str | None = None,
    task_name: str | None = None,
    task_sequence: int | None = None,
) -> tuple[dict, bool, float]:
    """
    Execute a research plan using research_workflow workflow.

    Uses research_workflow for automatic tool loop handling and forced output.

    Args:
        plan: The research plan to execute
        artifact_pool: Artifact pool for resolving dependencies
        config: Pipeline configuration
        run_dir: Run output directory
        iteration: Current invention loop iteration number
        research_idx: Index of this research within the iteration
        telemetry: Optional AIITelemetry for message logging
        task_id: Task ID for telemetry
        task_name: Task name for telemetry

    Returns:
        (result_dict, status, cost_usd)
    """
    # Create initializer and finalizer
    initializer = AgentInitializer(telemetry=telemetry, task_id=task_id, task_name=task_name)
    finalizer = AgentFinalizer(telemetry=telemetry, task_id=task_id, task_name=task_name)

    # Setup workspace
    workspace_dir = run_dir / (task_id or f"research_workspace_idx{research_idx}")
    initializer.setup_workspace(workspace_dir)

    # Start task (must be before any emit_message calls)
    initializer.start_task(sequence=task_sequence)

    # Log dependency info for debugging
    if plan.artifact_dependencies and telemetry:
        dep_artifacts = artifact_pool.get_by_ids(plan.artifact_dependencies)
        dep_info = [(d.id, d.title, bool(d.summary)) for d in dep_artifacts]
        telemetry.emit(MessageType.DEBUG, f"Research deps received: {dep_info}")

    research_cfg = config.invention_loop.execute.research
    use_claude_agent = research_cfg.use_claude_agent

    # =========================================================================
    # Route to Claude agent if enabled
    # =========================================================================
    if use_claude_agent:
        if telemetry and task_id and task_name:
            telemetry.emit_message("INFO", f"Executing RESEARCH (Claude agent): {plan.title}", task_name, task_id)
            telemetry.emit_message("INFO", f"Workspace: {workspace_dir}", task_name, task_id)

        return await _execute_research_claude_agent(
            plan=plan,
            artifact_pool=artifact_pool,
            config=config,
            run_dir=run_dir,
            workspace_dir=workspace_dir,
            iteration=iteration,
            research_idx=research_idx,
            finalizer=finalizer,
            telemetry=telemetry,
            task_id=task_id,
            task_name=task_name,
        )

    # =========================================================================
    # OpenRouter execution path
    # =========================================================================
    callback = None
    if telemetry and task_id and task_name:
        telemetry.emit_message("INFO", f"Executing RESEARCH: {plan.title}", task_name, task_id)
        telemetry.emit_message("INFO", f"Workspace: {workspace_dir}", task_name, task_id)
        callback = telemetry.create_callback(task_id, task_name, group="research")

    # Get OpenRouter config
    model = research_cfg.model
    if model == "claude-sonnet-4-5":
        raise ValueError(f"Model '{model}' is not available on OpenRouter — configure a valid OpenRouter model for research executor")
    reasoning_effort = research_cfg.reasoning_effort
    suffix = research_cfg.suffix
    llm_timeout = research_cfg.llm_timeout
    max_tool_iterations = research_cfg.max_tool_iterations
    openrouter_key = config.api_keys.openrouter

    if not openrouter_key:
        if telemetry and task_id and task_name:
            telemetry.emit_message("ERROR", "Missing OpenRouter API key", task_name, task_id)
        raise ValueError("Missing OpenRouter API key for research executor")

    # Build plan text from typed fields
    plan_text = plan.to_prompt_yaml()

    # Build prompt
    prompt = get_exec_prompt(
        plan_text=plan_text,
        artifact_pool=artifact_pool,
        dependency_ids=plan.artifact_dependencies,
    )
    system_prompt = get_exec_sysprompt()

    try:
        effective_model = OpenRouterClient.resolve_model(model, suffix)

        async with OpenRouterClient(
            api_key=openrouter_key,
            model=effective_model,
            timeout=llm_timeout,
        ) as client:
            # Use research_workflow workflow for automatic tool loop + structured output
            result = await research_workflow(
                client=client,
                prompt=prompt,
                system=system_prompt,
                config=ResearchWorkflowConfig(
                    max_tool_iterations=max_tool_iterations,
                    force_output_prompt=get_force_output_prompt(),
                    tools=RESEARCH_TOOLS,
                    timeout=llm_timeout,
                ),
                response_format=ResearchArtifact,  # Schema enforces output format
                message_callback=callback,
                reasoning_effort=reasoning_effort,
            )

            # Get answer from structured output only
            answer = None
            if result.output:
                answer = result.output.get("answer")

            if not answer:
                # Build detailed error info
                stats = result.tool_result.stats
                error_details = (
                    f"No output after {result.tool_result.iterations_used} iterations. "
                    f"Tools: {stats.tool_calls.get('aii_web_search_fast', 0)} searches, "
                    f"{stats.tool_calls.get('aii_web_fetch_direct', 0)} fetches. "
                    f"Forced: {result.forced_output}. "
                    f"Cost: ${stats.total_cost:.4f}"
                )
                finalizer.end_task_failure(error_details, cost=stats.total_cost)
                raise RuntimeError(f"Research executor produced no answer: {error_details}")

            # Get cost from tool loop result
            cost = result.tool_result.stats.total_cost

            # Extract fields from structured output
            title = ""
            sources = []
            follow_up_questions = []
            summary = ""
            if result.output:
                title = result.output.get("title", "")
                sources = result.output.get("sources", [])
                follow_up_questions = result.output.get("follow_up_questions", [])
                summary = result.output.get("summary", "")

            # Fallbacks
            if not title:
                title = plan.title.strip()
            if not summary:
                summary = f"Research: {answer[:180]}..." if len(answer) > 180 else f"Research: {answer}"

            # Build result
            research_result = {
                "question": plan.question or plan.research_plan,
                "answer": answer,
                "title": title,
                "sources": sources,
                "follow_up_questions": follow_up_questions,
                "summary": summary,
                "model": result.tool_result.stats.model or effective_model,
                "tool_calls": {
                    "aii_web_search_fast": result.tool_result.stats.tool_calls.get("aii_web_search_fast", 0),
                    "aii_web_fetch_direct": result.tool_result.stats.tool_calls.get("aii_web_fetch_direct", 0),
                },
                "iterations_used": result.tool_result.iterations_used,
                "forced_output": result.forced_output,
                "workspace_path": str(workspace_dir),
            }

            # Write output files to workspace
            # 1. research_out.json - structured output
            research_out_path = workspace_dir / "research_out.json"
            research_out_path.write_text(json.dumps(research_result, indent=2), encoding="utf-8")

            # 2. research_report.md (demo file)
            _write_research_report(research_result, workspace_dir)

            if telemetry and task_id and task_name:
                telemetry.emit_message("SUCCESS", f"Research complete: {len(answer)} chars, {len(sources)} sources, ${cost:.4f}", task_name, task_id)
            finalizer.end_task_success(cost=cost)
            return research_result, True, cost

    except asyncio.TimeoutError:
        finalizer.end_task_timeout(llm_timeout)
        raise
    except Exception as e:
        finalizer.end_task_error(str(e))
        raise
