{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Core ML paradigm that offline RL and the MOPO algorithm are built upon",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Model-based reinforcement learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "The base approach that MOPO modifies and improves for offline settings",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Model-free reinforcement learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Contrasted approach that constrains policy to data support; MOPO outperforms these methods",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)",
      "wikidata": {
        "id": "Q63788448",
        "label": "model-free reinforcement learning",
        "description": "type of machine learning algorithm",
        "aliases": [],
        "claims": {
          "google_kg_id": "/g/11fjzc3qzm",
          "subclass_of": {
            "id": "Q830687",
            "label": "reinforcement learning"
          },
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Markov decision process",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Theoretical framework used to formulate offline RL problem and analyze policy returns",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "wikidata": {
        "id": "Q176789",
        "label": "Markov decision process",
        "description": "mathematical model for sequential decision making under uncertainty",
        "aliases": [
          "MDP",
          "MDPs"
        ],
        "claims": {
          "P138": "Q176659",
          "freebase_id": "/m/048gl8",
          "P1051": "7713",
          "P4969": "Q176814",
          "mag_id": "106189395",
          "openalex_id": "C106189395",
          "subclass_of": [
            "Q1331926",
            {
              "id": "Q176737",
              "label": "stochastic process"
            }
          ],
          "openalex_topic_id": "56113",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Policy optimization",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "The proposed MOPO algorithm directly optimizes policies with uncertainty-penalized rewards",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Proximal_policy_optimization",
      "wikidata": {
        "id": "Q112150238",
        "label": "Proximal Policy Optimization",
        "description": "model-free reinforcement learning algorithm",
        "aliases": [
          "PPO"
        ],
        "claims": {
          "subclass_of": [
            {
              "id": "Q113840014",
              "label": "policy-gradient method"
            },
            {
              "id": "Q63788448",
              "label": "model-free reinforcement learning"
            }
          ],
          "P61": {
            "id": "Q21708200",
            "label": "OpenAI Foundation"
          }
        }
      }
    },
    {
      "name": "Uncertainty quantification",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "MOPO's key innovation: penalizing rewards by dynamics uncertainty to avoid distributional shift",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Uncertainty_quantification",
      "wikidata": {
        "id": "Q7882499",
        "label": "uncertainty quantification",
        "description": "characterization and reduction of uncertainties in both computational and real world applications",
        "aliases": [
          "UQ",
          "Uncertainty quantification model"
        ],
        "claims": {
          "subclass_of": "Q29205",
          "babelnet_id": "03426783n",
          "mag_id": "32230216",
          "P1269": {
            "id": "Q13649246",
            "label": "uncertainty"
          },
          "freebase_id": "/m/0fj8wc",
          "openalex_id": "C32230216"
        }
      }
    }
  ]
}