{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Language model",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "DPO fine-tunes unsupervised language models to align with human preferences without RLHF complexity.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model",
      "wikidata": {
        "id": "Q3621696",
        "label": "language model",
        "description": "probabilistic model of a natural or formal language, or generally of elements of signal sequences",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/065lv0",
          "mag_id": "137293760",
          "subclass_of": "Q3284399",
          "P1269": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "P8408": "LanguageModeling",
          "openalex_id": "C137293760",
          "P10407": "923-2",
          "use": "Q96407327",
          "openalex_topic_id": "184201",
          "topic_main_category": "Q16500316",
          "P12086": "Taalmodel",
          "P8309": "18-332728",
          "P1368": "000355288",
          "P13397": "language+model",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Reinforcement learning from human feedback",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "DPO is presented as an alternative to RLHF, a standard method for aligning language models with human preferences.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback",
      "wikidata": {
        "id": "Q115570683",
        "label": "reinforcement learning from human feedback",
        "description": "variant of reinforcement learning",
        "aliases": [
          "RLHF"
        ],
        "claims": {
          "subclass_of": {
            "id": "Q830687",
            "label": "reinforcement learning"
          },
          "uses": "Q5",
          "described_by_source": "Q104090584",
          "instance_of": "Q117348143",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Reinforcement learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "DPO eliminates the need for reinforcement learning to maximize estimated rewards during fine-tuning.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Sentiment analysis",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "DPO exceeds PPO-based RLHF in ability to control sentiment of language model generations.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Sentiment_analysis",
      "wikidata": {
        "id": "Q2271421",
        "label": "sentiment analysis",
        "description": "use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials",
        "aliases": [
          "opinion mining",
          "Opinion Extraction"
        ],
        "claims": {
          "freebase_id": "/m/0g57xn",
          "quora_topic_id": "Sentiment-Analysis",
          "P3827": "sentiment-analysis",
          "subclass_of": [
            "Q1185804",
            {
              "id": "Q30642",
              "label": "natural language processing"
            }
          ],
          "commons_category": "Sentiment analysis",
          "P2179": "10003353",
          "P2579": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "use": "Q39809",
          "instance_of": {
            "id": "Q627436",
            "label": "field of work"
          },
          "mag_id": "66402592",
          "stack_exchange_tag": [
            "https://datascience.stackexchange.com/tags/sentiment-analysis",
            "https://stackoverflow.com/tags/sentiment-analysis"
          ],
          "uses": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "P3553": "19966010",
          "P9100": "sentiment-analysis",
          "openalex_id": "C66402592",
          "mesh_id": "D000090042",
          "mesh_tree_code": [
            "G17.035.250.750",
            "L01.224.050.375.815",
            "L01.313.500.750.280.199.750",
            "L01.470.625.750"
          ],
          "openalex_topic_id": "219676",
          "P7502": "Sentiment_analysis-V48YAR",
          "P2892": "C5544431",
          "P9309": "sentimentAnalysis",
          "P8168": "Q1194520",
          "P13397": "sentiment+analysis"
        }
      }
    },
    {
      "name": "Automatic summarization",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Summarization is one of the key tasks where DPO matches or improves response quality over RLHF.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Automatic_summarization",
      "wikidata": {
        "id": "Q1394144",
        "label": "automatic summarization",
        "description": "computer-based method for shortening a text",
        "aliases": [
          "text summarization",
          "summarization",
          "text summarisation",
          "automatic text summarization",
          "email summarisation",
          "message summarization"
        ],
        "claims": {
          "freebase_id": "/m/02z9hx",
          "stack_exchange_tag": "https://stackoverflow.com/tags/summarization",
          "quora_topic_id": "Automatic-Summarization",
          "subclass_of": [
            "Q676880",
            {
              "id": "Q30642",
              "label": "natural language processing"
            }
          ],
          "P2579": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "mag_id": "170858558",
          "openalex_id": "C170858558",
          "openalex_topic_id": [
            "154581",
            "26832"
          ]
        }
      }
    },
    {
      "name": "Dialogue system",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Single-turn dialogue is one of the main tasks where DPO demonstrates improvements in response quality.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Dialogue_system",
      "wikidata": {
        "id": "Q5270587",
        "label": "dialogue system",
        "description": "computer system intended to converse with a human",
        "aliases": [
          "dialog system",
          "conversational agent",
          "man-machine conversation"
        ],
        "claims": {
          "subclass_of": [
            "Q121182",
            {
              "id": "Q30642",
              "label": "natural language processing"
            }
          ],
          "mag_id": "190954187",
          "freebase_id": "/m/08y96v",
          "openalex_id": "C190954187",
          "P691": "ph351273",
          "google_kg_id": "/g/121hrdl8"
        }
      }
    },
    {
      "name": "Preference learning",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "DPO is a novel preference optimization method that extracts optimal policies from reward models using preference-based learning instead of reinforcement learning.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Preference_learning",
      "wikidata": {
        "id": "Q7239820",
        "label": "preference learning",
        "description": "Subfield of machine learning",
        "aliases": [],
        "claims": {
          "mag_id": "181204326",
          "freebase_id": "/m/0hq_dn9",
          "openalex_id": "C181204326",
          "subclass_of": "Q2539",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    }
  ]
}