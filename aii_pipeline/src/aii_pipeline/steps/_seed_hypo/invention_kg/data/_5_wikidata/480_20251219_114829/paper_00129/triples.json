{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Large language model",
      "relation": "proposes",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "relevance": "Gemma is a new family of lightweight large language models that represents a novel contribution to open-source LLM development.",
      "wikidata": {
        "id": "Q115305900",
        "label": "large language model",
        "description": "language model built with very large amounts of texts",
        "aliases": [
          "LLM",
          "LLMs",
          "large language models",
          "word soup machine",
          "word soup model",
          "word salad machine",
          "word salad model"
        ],
        "claims": {
          "subclass_of": "Q3621696",
          "P1269": {
            "id": "Q11660",
            "label": "artificial intelligence"
          },
          "short_name": "LLM",
          "uses": [
            {
              "id": "Q117217619",
              "label": "AI prompt"
            },
            "Q85810444"
          ],
          "P5008": "Q117245199",
          "topic_main_category": {
            "id": "Q117280639",
            "label": "Category:Large language models"
          },
          "has_parts": [
            {
              "id": "Q116777014",
              "label": "generative pre-trained transformer"
            },
            "Q117246174",
            "Q3614994"
          ],
          "P973": "https://www.youtube.com/watch?v=WqYBx2gB6vA",
          "P5844": "neo-modello-linguistico-di-grandi-dimensioni_(Neologismi)",
          "opposite_of": "Q123759530",
          "google_kg_id": "/g/11kc9956b3",
          "gnd_id": "1322631905",
          "P12861": "E434",
          "use": [
            "Q116961403",
            {
              "id": "Q3510521",
              "label": "computer security"
            }
          ],
          "instance_of": {
            "id": "Q117349475",
            "label": "artificial intelligence model type"
          },
          "commons_category": "Large language models",
          "britannica_id": "topic/large-language-model",
          "P989": [
            "Wikipedia - Large language model (spoken by AI voice).mp3",
            "Wikipedia - Modelo extenso de lenguaje (hablado por voz AI).mp3"
          ],
          "mesh_id": "D000098342",
          "mesh_tree_code": [
            "G17.035.250.500.250.500",
            "G17.485.500.500",
            "L01.224.050.375.530.250.500",
            "L01.224.050.375.605.500.500",
            "L01.224.494"
          ],
          "P3911": "30478-6",
          "P2354": {
            "id": "Q131598147",
            "label": "list of large language models"
          },
          "P1368": "000355290",
          "described_by_source": "Q133280509",
          "P508": "77644",
          "P13572": "large+language+model",
          "stack_exchange_tag": [
            "https://genai.stackexchange.com/tags/llm",
            "https://ai.stackexchange.com/tags/large-language-models",
            "https://stats.stackexchange.com/tags/llm",
            "https://stackoverflow.com/tags/large-language-model",
            "https://datascience.stackexchange.com/tags/llm",
            "https://softwarerecs.stackexchange.com/tags/llm",
            "https://mathematica.stackexchange.com/tags/llm-functions",
            "https://or.stackexchange.com/tags/llm",
            "https://law.stackexchange.com/tags/llm",
            "https://hermeneutics.stackexchange.com/tags/llm"
          ],
          "P9100": "llm",
          "P691": "ph1274563"
        }
      }
    },
    {
      "name": "Language model",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model",
      "relevance": "Language modeling is the foundational concept underlying Gemma's architecture and training approach.",
      "wikidata": {
        "id": "Q3621696",
        "label": "language model",
        "description": "probabilistic model of a natural or formal language, or generally of elements of signal sequences",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/065lv0",
          "mag_id": "137293760",
          "subclass_of": "Q3284399",
          "P1269": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "P8408": "LanguageModeling",
          "openalex_id": "C137293760",
          "P10407": "923-2",
          "use": "Q96407327",
          "openalex_topic_id": "184201",
          "topic_main_category": "Q16500316",
          "P12086": "Taalmodel",
          "P8309": "18-332728",
          "P1368": "000355288",
          "P13397": "language+model",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Natural language processing",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
      "relevance": "Gemma is designed to solve NLP tasks including language understanding and reasoning.",
      "wikidata": {
        "id": "Q30642",
        "label": "natural language processing",
        "description": "field of computer science and linguistics",
        "aliases": [
          "NLP"
        ],
        "claims": {
          "P349": "00562347",
          "topic_main_category": {
            "id": "Q9149760",
            "label": "Category:Natural language processing"
          },
          "P1051": "12529",
          "commons_category": "Natural language processing",
          "subclass_of": [
            {
              "id": "Q11660",
              "label": "artificial intelligence"
            },
            "Q21198",
            "Q182557",
            "Q11862829",
            "Q750843"
          ],
          "mesh_id": "D009323",
          "P3827": "natural-language-processing",
          "study_of": [
            {
              "id": "Q2554325",
              "label": "lemmatisation"
            },
            {
              "id": "Q1271424",
              "label": "part-of-speech tagging"
            },
            "Q194152",
            {
              "id": "Q7451191",
              "label": "sentence boundary disambiguation"
            },
            {
              "id": "Q1416732",
              "label": "stemming"
            },
            {
              "id": "Q2748079",
              "label": "terminology extraction"
            },
            {
              "id": "Q1759657",
              "label": "lexical semantics"
            },
            "Q79798",
            "Q403574",
            {
              "id": "Q1513879",
              "label": "natural language generation"
            },
            "Q167555",
            "Q1074173",
            "Q6588467",
            {
              "id": "Q7310755",
              "label": "relationship extraction"
            },
            {
              "id": "Q2271421",
              "label": "sentiment analysis"
            },
            "Q1948408",
            "Q48522",
            {
              "id": "Q1394144",
              "label": "automatic summarization"
            },
            "Q63087",
            {
              "id": "Q1129466",
              "label": "discourse analysis"
            },
            {
              "id": "Q189436",
              "label": "speech recognition"
            },
            {
              "id": "Q2266173",
              "label": "speech segmentation"
            },
            {
              "id": "Q16346",
              "label": "speech synthesis"
            },
            "Q18395344",
            {
              "id": "Q47165059",
              "label": "decompounding"
            },
            {
              "id": "Q2438971",
              "label": "tokenization"
            }
          ],
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/nlp",
            "https://ai.stackexchange.com/tags/natural-language-processing"
          ],
          "britannica_id": "technology/natural-language-processing-computer-science",
          "P3219": "traitement-automatique-des-langues",
          "loc_id": "sh88002425",
          "P3235": "natural-language-processing",
          "P5922": "080107",
          "P1296": "0281325",
          "short_name": [
            "NLP",
            "TAL",
            "TALN",
            "PLN",
            "PLN",
            "PLN",
            "NLP",
            "PIN"
          ],
          "P7502": "Natural_language_processing_(NLP)-DZY",
          "P3984": "LanguageTechnology",
          "topic_maintained_by": {
            "id": "Q23006254",
            "label": "Template:Natural language processing"
          },
          "P9100": [
            "natural-language-processing",
            "nlp"
          ],
          "freebase_id": "/m/05flf",
          "P9545": "249047",
          "P3553": "19560026",
          "P443": "LL-Q150 (fra)-Visiteur Journée 2 - 18 (Madehub)-traitement automatique du langage naturel.wav",
          "P8189": "987007536703305171",
          "quora_topic_id": "Natural-Language-Processing",
          "P8529": "460208",
          "openalex_id": "C204321447",
          "P4644": "fff0e2cd-d0bd-4b02-9daf-158b79d9688a",
          "mesh_tree_code": "L01.224.050.375.580",
          "openalex_topic_id": "91728",
          "P691": "ph427562",
          "P3847": [
            "natural_language_processing_(computer_science)",
            "natural_language_processing"
          ],
          "P2572": "NLProc",
          "practiced_by": {
            "id": "Q116952787",
            "label": "natural language processing engineer"
          },
          "P2892": "C0027489",
          "P9309": "naturalLanguageProcessing",
          "instance_of": [
            "Q11862829",
            {
              "id": "Q1047113",
              "label": "field of study"
            },
            "Q2267705",
            {
              "id": "Q268592",
              "label": "industry"
            },
            {
              "id": "Q135892289",
              "label": "branch of linguistics"
            }
          ],
          "P5844": "neo-elaborazione-del-linguaggio-naturale_(Neologismi)",
          "P12385": "processament-del-llenguatge-natural",
          "P5437": "c_67092197",
          "image": "T-SNE visualisation of word embeddings generated using 19th century literature.png",
          "P13397": "NLP",
          "P13591": "concept/e0fc4a1c-7602-43f9-9cea-0d8bb49039b8"
        }
      }
    },
    {
      "name": "Fine-tuning",
      "relation": "proposes",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
      "relevance": "The paper provides fine-tuned checkpoints of Gemma models, representing a novel application of fine-tuning techniques.",
      "wikidata": {
        "id": "Q117286419",
        "label": "fine-tuning",
        "description": "process of taking a pre-trained model and further training it on a smaller, specific dataset to adapt or improve its performance for a particular task or domain",
        "aliases": [
          "neural network fine-tuning"
        ],
        "claims": {
          "instance_of": "Q117348143",
          "commons_category": "AI model fine-tuning",
          "follows": {
            "id": "Q124149651",
            "label": "pre-training"
          },
          "subclass_of": "Q1714153",
          "P10": "Easiest Way to Build Consistent Character - Step-by-Step Guide with OpenArt.webm"
        }
      }
    },
    {
      "name": "Transfer learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transfer_learning",
      "relevance": "Gemma leverages transfer learning by building on research and technology from Gemini models.",
      "wikidata": {
        "id": "Q6027324",
        "label": "transfer learning",
        "description": "research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem",
        "aliases": [
          "inductive transfer"
        ],
        "claims": {
          "subclass_of": "Q2539",
          "quora_topic_id": "Transfer-Learning",
          "part_of": "Q2539",
          "mag_id": "77075516",
          "freebase_id": "/m/0b6vrv",
          "instance_of": {
            "id": "Q28643",
            "label": "paradigm"
          },
          "uses": {
            "id": "Q6015447",
            "label": "incremental computation"
          },
          "P9100": "transfer-learning",
          "openalex_id": "C77075516",
          "schematic": "Transfer learning.svg",
          "different_from": "Q1820378",
          "mesh_id": "D000098410",
          "mesh_tree_code": [
            "G17.035.250.500.625",
            "L01.224.050.375.530.625"
          ],
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Generative pre-trained transformer",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
      "relevance": "Gemma likely uses transformer-based architecture similar to other modern language models in this category.",
      "wikidata": {
        "id": "Q116777014",
        "label": "generative pre-trained transformer",
        "description": "type of large language model",
        "aliases": [
          "GPT",
          "generative pretrained transformer"
        ],
        "claims": {
          "has_parts": [
            {
              "id": "Q116937684",
              "label": "GPT-J"
            },
            "Q115564437",
            {
              "id": "Q112702082",
              "label": "foundation model"
            },
            "Q116709136",
            {
              "id": "Q95726718",
              "label": "GPT-1"
            },
            "Q95726734",
            "Q95726727",
            {
              "id": "Q117791942",
              "label": "AutoGPT"
            },
            "Q116793893",
            {
              "id": "Q105078662",
              "label": "DALL-E"
            },
            "Q108582200",
            {
              "id": "Q118176939",
              "label": "Open Assistant"
            }
          ],
          "P5008": "Q117245199",
          "subclass_of": [
            {
              "id": "Q115305900",
              "label": "large language model"
            },
            "Q117246174",
            "Q85810444",
            "Q2539",
            {
              "id": "Q11660",
              "label": "artificial intelligence"
            }
          ],
          "different_from": "Q124237017",
          "short_name": "GPT",
          "google_kg_id": "/g/11ts8q7891",
          "P8313": "GPT",
          "P5019": "generative-pre-trained-transformer-informatik",
          "P11662": "3641756"
        }
      }
    },
    {
      "name": "Transformer",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
      "relevance": "The transformer architecture is the underlying neural network architecture for modern LLMs like Gemma.",
      "wikidata": {
        "id": "Q85810444",
        "label": "transformer",
        "description": "machine-learning model architecture first developed by Google Brain",
        "aliases": [
          "transformer model",
          "transformer architecture",
          "transformers"
        ],
        "claims": {
          "subclass_of": [
            "Q192776",
            {
              "id": "Q113364611",
              "label": "deep learning model"
            }
          ],
          "developer": [
            {
              "id": "Q16927616",
              "label": "Google Brain"
            },
            {
              "id": "Q44749723",
              "label": "Ashish Vaswani"
            },
            "Q30251943"
          ],
          "follows": [
            {
              "id": "Q1457734",
              "label": "recurrent neural network"
            },
            {
              "id": "Q6673524",
              "label": "long short-term memory"
            }
          ],
          "described_by_source": {
            "id": "Q30249683",
            "label": "Attention Is All You Need"
          },
          "google_kg_id": "/g/11hz_m4ssw",
          "has_parts": [
            "Q42586063",
            {
              "id": "Q745243",
              "label": "decoder"
            }
          ],
          "uses": [
            {
              "id": "Q103701642",
              "label": "attention"
            },
            {
              "id": "Q5441227",
              "label": "feedforward neural network"
            }
          ],
          "use": [
            {
              "id": "Q30642",
              "label": "natural language processing"
            },
            {
              "id": "Q844240",
              "label": "computer vision"
            },
            {
              "id": "Q3245113",
              "label": "statistical machine translation"
            },
            {
              "id": "Q1394144",
              "label": "automatic summarization"
            }
          ],
          "schematic": "Transformer, full architecture.png",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/transformer",
          "P575": "2017-06-12",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Attention mechanism",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
      "relevance": "Attention mechanisms are a core component of transformer-based language models like Gemma.",
      "wikidata": {
        "id": "Q103701642",
        "label": "attention",
        "description": "machine learning technique",
        "aliases": [
          "attention mechanism"
        ],
        "claims": {
          "subclass_of": "Q6501338",
          "P1269": "Q2539",
          "google_kg_id": "/g/11qpclnpwr",
          "use": [
            "Q192776",
            {
              "id": "Q25325414",
              "label": "neural Turing machine"
            },
            {
              "id": "Q28324912",
              "label": "differentiable neural computer"
            },
            "Q85810444",
            "Q108281205"
          ],
          "P1557": "Q6501338",
          "openalex_topic_id": "413584",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/attention",
          "schematic": "Attention mechanism overview.svg"
        }
      }
    },
    {
      "name": "Deep learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Deep_learning",
      "relevance": "Deep learning is the fundamental machine learning approach used to train Gemma models.",
      "wikidata": {
        "id": "Q197536",
        "label": "deep learning",
        "description": "branch of machine learning",
        "aliases": [
          "DL",
          "deep machine learning",
          "hierarchical learning",
          "deep structured learning"
        ],
        "claims": {
          "freebase_id": "/m/0h1fn8h",
          "P61": [
            {
              "id": "Q3571662",
              "label": "Yann Le Cun"
            },
            {
              "id": "Q92894",
              "label": "Geoffrey Hinton"
            }
          ],
          "topic_main_category": {
            "id": "Q24070291",
            "label": "Category:Deep learning"
          },
          "quora_topic_id": "Deep-Learning",
          "P3219": "apprentissage-profond-deep-learning",
          "subclass_of": {
            "id": "Q111862379",
            "label": "machine learning method"
          },
          "commons_category": "Deep learning",
          "P4342": [
            "dyplæring",
            "dyp_læring"
          ],
          "P6363": "http://data.thenextweb.com/tnw/entity/deep_learning",
          "use": {
            "id": "Q11660",
            "label": "artificial intelligence"
          },
          "P3553": "19813032",
          "mesh_id": "D000077321",
          "mesh_tree_code": [
            "G17.035.250.500.250",
            "G17.485.500",
            "L01.224.050.375.530.250",
            "L01.224.050.375.605.500"
          ],
          "mag_id": "108583219",
          "P3984": "deeplearning",
          "P7502": "Deep_learning-PE5E9Y",
          "P9100": [
            "deep-learning",
            "deep-learning-tutorial",
            "deeplearning"
          ],
          "P8529": "461103",
          "P9526": "Deep_Learning",
          "image": "Deep Learning.jpg",
          "P6802": "Computer vision sample in Simón Bolivar Avenue, Quito.jpg",
          "openalex_id": [
            "C108583219",
            "C2984842247"
          ],
          "P2347": "39324",
          "openalex_topic_id": [
            "216836",
            "221130"
          ],
          "P691": "ph1042930",
          "P11408": "ディープラーニング",
          "P8687": "+95159",
          "P5844": "deep-learning_(Neologismi)",
          "P2892": "C4704761",
          "P11196": "深度学习",
          "P12086": "Deep_learning",
          "P8189": "987011038167805171",
          "loc_id": "sh2021006947",
          "P1368": "000350792",
          "P9346": "deep-learning",
          "P3911": "30476-3",
          "P6900": "ディープラーニング",
          "P13397": "deep+learning",
          "P508": "78027",
          "P268": "17706295v",
          "gnd_id": "1135597375",
          "P3235": "deep-learning",
          "stack_exchange_tag": [
            "https://ai.stackexchange.com/tags/deep-learning",
            "https://or.stackexchange.com/tags/deep-learning",
            "https://stats.stackexchange.com/tags/deep-learning"
          ],
          "P13591": "concept/21bbd7b7-62b1-4bd8-844d-00f4bc0ccd62",
          "P1036": "006.31",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Neural network",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
      "relevance": "Neural networks form the computational foundation of language models like Gemma.",
      "wikidata": {
        "id": "Q192776",
        "label": "artificial neural network",
        "description": "computational model used in machine learning, based on connected, hierarchical functions",
        "aliases": [
          "ANN",
          "neural network",
          "connectionist system",
          "connectionist systems",
          "deep nets",
          "Simulated Neural Network",
          "Static Neural Network",
          "SNN",
          "Synergetic Neural Network",
          "Neural Networks, Computer",
          "neural net",
          "artificial neural networks"
        ],
        "claims": {
          "P349": "01165604",
          "commons_category": "Artificial neural networks",
          "P1014": "300389897",
          "topic_main_category": "Q9246857",
          "P935": "Artificial neural network",
          "gnd_id": "4226127-2",
          "subclass_of": [
            {
              "id": "Q11660",
              "label": "artificial intelligence"
            },
            "Q2539",
            {
              "id": "Q12811862",
              "label": "neural network"
            },
            {
              "id": "Q5282087",
              "label": "discriminative model"
            }
          ],
          "mesh_id": "D016571",
          "freebase_id": "/m/05dhw",
          "quora_topic_id": "Artificial-Neural-Networks-ANNs",
          "P2924": [
            "4114009",
            "2256451"
          ],
          "P3827": "artificial-neural-networks",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/neural-network",
            "https://stats.stackexchange.com/tags/neural-networks",
            "https://cs.stackexchange.com/tags/neural-networks",
            "https://ai.stackexchange.com/tags/neural-networks"
          ],
          "babelnet_id": "00057388n",
          "P4732": "AT06947",
          "P3219": "reseaux-de-neurones-formels",
          "P4342": "nevralt_nettverk",
          "P5398": "000000120",
          "loc_id": "sh90001937",
          "P6564": "artificial-neural-network",
          "P443": "LL-Q150 (fra)-0x010C-réseau de neurones artificiels.wav",
          "P1245": "1706096",
          "mag_id": "50644808",
          "image": "Neural network.svg",
          "P1552": {
            "id": "Q7860946",
            "label": "types of artificial neural networks"
          },
          "P8408": "ArtificialNeuralNetwork",
          "mesh_tree_code": [
            "G17.485",
            "L01.224.050.375.605"
          ],
          "P7502": "Artificial_neural_network-YZ9",
          "P9100": [
            "neural-network",
            "artificial-neural-network"
          ],
          "P9545": "226614",
          "P9935": "reti-neurali-e-robotica",
          "P2004": "12606",
          "P8189": "987007551192405171",
          "P8529": "461104",
          "openalex_id": "C50644808",
          "has_parts": [
            "Q110857102",
            "Q1036748",
            {
              "id": "Q110857125",
              "label": "optimizer"
            },
            {
              "id": "Q177058",
              "label": "artificial neuron"
            },
            {
              "id": "Q101517027",
              "label": "neural processing unit"
            }
          ],
          "openalex_topic_id": [
            "18651",
            "509993",
            "238094",
            "88631"
          ],
          "P1557": "Q43283",
          "uses": "Q4677469",
          "P691": "ph115443",
          "P6262": "apple:Artificial_neural_network",
          "P3847": "neural_networks_(computer_science)",
          "P1269": [
            {
              "id": "Q11660",
              "label": "artificial intelligence"
            },
            "Q2539"
          ],
          "P11514": "neironnye-seti-e734b3",
          "P10376": [
            "computer-science/artificial-neural-networks",
            "computer-science/artificial-neural-network",
            "biochemistry-genetics-and-molecular-biology/artificial-neural-network",
            "veterinary-science-and-veterinary-medicine/artificial-neural-network",
            "physics-and-astronomy/artificial-neural-network",
            "nursing-and-health-professions/artificial-neural-network",
            "neuroscience/artificial-neural-network",
            "mathematics/artificial-neural-network",
            "immunology-and-microbiology/artificial-neural-network",
            "engineering/artificial-neural-network",
            "earth-and-planetary-sciences/artificial-neural-network"
          ],
          "P2892": "C0870951",
          "P7305": "3946887",
          "instance_of": "Q120281892",
          "P3911": "19808-6",
          "P5019": "neuronale-netze-kunstliche-intelligenz",
          "P1368": "000053170",
          "P5437": "c_65b9cd79",
          "P8885": "인공신경망",
          "P12946": "artificial+neural+network",
          "P7276": "UM%C4%9AL%C3%81%20NEURONOV%C3%81%20S%C3%8D%C5%A4",
          "P6900": "ニューラルネットワーク",
          "P3365": "rete-neurale",
          "P9621": "rete-neurale",
          "P10037": "reti-neurali-e-vita-artificiale",
          "P9941": "reti-neurali",
          "P13411": "Q153",
          "P989": "En-Neural network.ogg",
          "P13591": "concept/6faed690-ea28-4208-a446-ad355ec1cb8e",
          "P10380": "network-models"
        }
      }
    },
    {
      "name": "Document classification",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Document_classification",
      "relevance": "Document/text classification is one of the language understanding tasks Gemma is evaluated on.",
      "wikidata": {
        "id": "Q302088",
        "label": "document classification",
        "description": "problem in library science, information science and computer science",
        "aliases": [
          "document categorization",
          "text categorization"
        ],
        "claims": {
          "subclass_of": "Q1744628",
          "freebase_id": "/m/04tc3v",
          "stack_exchange_tag": "https://stackoverflow.com/tags/document-classification",
          "opposite_of": "Q4223102",
          "mag_id": "2780479914",
          "P3553": "19602596",
          "P9100": "document-classification",
          "google_kg_id": "/g/1q2xhffmp",
          "different_from": "Q846498",
          "openalex_id": [
            "C2780479914",
            "C2986744138"
          ],
          "openalex_topic_id": "59157",
          "P4644": "93aaf65c-5aad-4e6f-a7ba-ce9bf7c0e6e6",
          "P13397": "document+tagging",
          "P12913": {
            "id": "Q49848",
            "label": "document"
          }
        }
      }
    }
  ]
}