{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Prompt engineering",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "AutoPrompt is an automated approach to prompt engineering for generating effective task-specific prompts without manual effort.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Prompt_engineering",
      "wikidata": {
        "id": "Q108941486",
        "label": "prompt engineering",
        "description": "creation or optimization of a prompt to be given to an artificial intelligence model",
        "aliases": [
          "prompt-based learning",
          "AI prompt engineering"
        ],
        "claims": {
          "subclass_of": [
            "Q110484020",
            {
              "id": "Q80006",
              "label": "computer programming"
            }
          ],
          "instance_of": [
            {
              "id": "Q2695280",
              "label": "technique"
            },
            "Q627436"
          ],
          "P1269": "Q11660",
          "use": [
            "Q65066631",
            "Q117246174"
          ],
          "P1056": {
            "id": "Q117217619",
            "label": "AI prompt"
          },
          "P5008": "Q117245199",
          "practiced_by": [
            "Q117674392",
            {
              "id": "Q123653520",
              "label": "AI prompter"
            }
          ],
          "commons_category": "Prompt engineering for generative AI",
          "google_kg_id": "/g/11p6kpgt_n",
          "P691": "ph1266210"
        }
      }
    },
    {
      "name": "Language model",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "The paper studies what knowledge pretrained language models learn and how to elicit it through automated prompts.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model",
      "wikidata": {
        "id": "Q3621696",
        "label": "language model",
        "description": "probabilistic model of a natural or formal language, or generally of elements of signal sequences",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/065lv0",
          "mag_id": "137293760",
          "subclass_of": "Q3284399",
          "P1269": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "P8408": "LanguageModeling",
          "openalex_id": "C137293760",
          "P10407": "923-2",
          "use": "Q96407327",
          "openalex_topic_id": "184201",
          "topic_main_category": "Q16500316",
          "P12086": "Taalmodel",
          "P8309": "18-332728",
          "P1368": "000355288",
          "P13397": "language+model",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Cloze test",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "The paper reformulates tasks as fill-in-the-blanks problems (cloze tests) to gauge knowledge in pretrained language models.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Cloze_test",
      "wikidata": {
        "id": "Q951968",
        "label": "cloze test",
        "description": "\"Fill-in-the-blank\" language learning technique",
        "aliases": [
          "cloze deletion test",
          "cloze deletion",
          "Cloze procedure"
        ],
        "claims": {
          "mag_id": "2779286901",
          "openalex_id": "C2779286901",
          "freebase_id": "/m/027d7fs",
          "image": "Student matching cloze terms on smartboard.jpg",
          "subclass_of": "Q27318",
          "P9497": "4615",
          "commons_category": "Cloze test"
        }
      }
    },
    {
      "name": "Gradient descent",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "AutoPrompt uses gradient-guided search to automatically generate effective prompts.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Gradient_descent",
      "wikidata": {
        "id": "Q1199743",
        "label": "gradient descent",
        "description": "optimization algorithm",
        "aliases": [
          "steepest descent",
          "method of steepest descent",
          "hill-climbing algorithm"
        ],
        "claims": {
          "freebase_id": "/m/01cmhh",
          "commons_category": "Gradient descent",
          "quora_topic_id": "Gradient-Descent",
          "instance_of": [
            "Q2835765",
            "Q2321565"
          ],
          "P2534": "\\mathbf{x}_{n+1} = \\mathbf{x}_n -\\gamma\\nabla F(\\mathbf{x}_n)",
          "uses": "Q1868524",
          "mag_id": "153258448",
          "P2579": "Q141495",
          "P9100": "gradient-descent",
          "openalex_id": "C153258448",
          "openalex_topic_id": "413756",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "subclass_of": "Q24034552",
          "use": {
            "id": "Q2539",
            "label": "machine learning"
          },
          "P2812": "MethodofSteepestDescent"
        }
      }
    },
    {
      "name": "BERT (language model)",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "The paper evaluates masked language models like BERT for performing various NLP tasks without fine-tuning.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
      "wikidata": {
        "id": "Q61726893",
        "label": "bidirectional encoder representations from transformers",
        "description": "deep learning artificial neural network language model",
        "aliases": [
          "BERT",
          "bidirectional encoder representations from transformer"
        ],
        "claims": {
          "described_by_source": {
            "id": "Q57267388",
            "label": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
          },
          "P571": "2018-00-00",
          "P3575": [
            "+110000000",
            "+340000000"
          ],
          "instance_of": [
            "Q115305900",
            "Q85810444",
            "Q24868018"
          ],
          "developer": {
            "id": "Q28943742",
            "label": "Google Research"
          },
          "P1324": "https://github.com/google-research/bert",
          "P275": "Q13785927",
          "use": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "google_kg_id": "/g/11h75tkhxs",
          "P9100": "bert",
          "P138": "Q584184",
          "P6216": {
            "id": "Q50423863",
            "label": "copyrighted"
          },
          "P973": [
            "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html",
            "https://devopedia.org/bert-language-model"
          ],
          "subclass_of": "Q85810444",
          "P856": "https://arxiv.org/abs/1810.04805",
          "commons_category": "BERT"
        }
      }
    },
    {
      "name": "Sentiment analysis",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "The paper demonstrates that MLMs can perform sentiment analysis through AutoPrompt-generated prompts.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Sentiment_analysis",
      "wikidata": {
        "id": "Q2271421",
        "label": "sentiment analysis",
        "description": "use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials",
        "aliases": [
          "opinion mining",
          "Opinion Extraction"
        ],
        "claims": {
          "freebase_id": "/m/0g57xn",
          "quora_topic_id": "Sentiment-Analysis",
          "P3827": "sentiment-analysis",
          "subclass_of": [
            "Q1185804",
            {
              "id": "Q30642",
              "label": "natural language processing"
            }
          ],
          "commons_category": "Sentiment analysis",
          "P2179": "10003353",
          "P2579": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "use": "Q39809",
          "instance_of": "Q627436",
          "mag_id": "66402592",
          "stack_exchange_tag": [
            "https://datascience.stackexchange.com/tags/sentiment-analysis",
            "https://stackoverflow.com/tags/sentiment-analysis"
          ],
          "uses": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "P3553": "19966010",
          "P9100": "sentiment-analysis",
          "openalex_id": "C66402592",
          "mesh_id": "D000090042",
          "mesh_tree_code": [
            "G17.035.250.750",
            "L01.224.050.375.815",
            "L01.313.500.750.280.199.750",
            "L01.470.625.750"
          ],
          "openalex_topic_id": "219676",
          "P7502": "Sentiment_analysis-V48YAR",
          "P2892": "C5544431",
          "P9309": "sentimentAnalysis",
          "P8168": {
            "id": "Q1194520",
            "label": "graft-versus-host disease"
          },
          "P13397": "sentiment+analysis"
        }
      }
    },
    {
      "name": "Textual entailment",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Natural language inference (textual entailment) is evaluated as one of the tasks that MLMs can perform with AutoPrompt.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Textual_entailment",
      "wikidata": {
        "id": "Q6588467",
        "label": "textual entailment",
        "description": "concept in natural language processing",
        "aliases": [
          "natural language inference"
        ],
        "claims": {
          "P2579": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "mag_id": "95318506",
          "freebase_id": "/m/0h3mz8c",
          "openalex_id": "C95318506",
          "P3712": {
            "id": "Q1394144",
            "label": "automatic summarization"
          }
        }
      }
    },
    {
      "name": "Relationship extraction",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "The paper shows that MLMs can be used as relation extractors more effectively than supervised relation extraction models using AutoPrompt.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Relationship_extraction",
      "wikidata": {
        "id": "Q7310755",
        "label": "relationship extraction",
        "description": "type of text mining",
        "aliases": [
          "relation extraction",
          "Relation Extraction",
          "relational extraction"
        ],
        "claims": {
          "mag_id": "153604712",
          "subclass_of": {
            "id": "Q676880",
            "label": "text mining"
          },
          "freebase_id": "/m/02rhpj1",
          "openalex_id": "C153604712",
          "openalex_topic_id": "443370",
          "instance_of": [
            {
              "id": "Q11862829",
              "label": "academic discipline"
            },
            {
              "id": "Q1047113",
              "label": "field of study"
            },
            {
              "id": "Q2267705",
              "label": "field of study"
            }
          ]
        }
      }
    },
    {
      "name": "Fine-tuning (deep learning)",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "AutoPrompt provides a parameter-free alternative to fine-tuning, a traditional approach for adapting language models to specific tasks.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
      "wikidata": {
        "id": "Q117286419",
        "label": "fine-tuning",
        "description": "process of taking a pre-trained model and further training it on a smaller, specific dataset to adapt or improve its performance for a particular task or domain",
        "aliases": [
          "neural network fine-tuning"
        ],
        "claims": {
          "instance_of": "Q117348143",
          "commons_category": "AI model fine-tuning",
          "follows": {
            "id": "Q124149651",
            "label": "pre-training"
          },
          "subclass_of": {
            "id": "Q1714153",
            "label": "adjustment"
          },
          "P10": "Easiest Way to Build Consistent Character - Step-by-Step Guide with OpenArt.webm"
        }
      }
    },
    {
      "name": "Transfer learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper leverages transfer learning principles by eliciting knowledge from pretrained language models without additional parameters.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transfer_learning",
      "wikidata": {
        "id": "Q6027324",
        "label": "transfer learning",
        "description": "research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem",
        "aliases": [
          "inductive transfer"
        ],
        "claims": {
          "subclass_of": {
            "id": "Q2539",
            "label": "machine learning"
          },
          "quora_topic_id": "Transfer-Learning",
          "part_of": {
            "id": "Q2539",
            "label": "machine learning"
          },
          "mag_id": "77075516",
          "freebase_id": "/m/0b6vrv",
          "instance_of": {
            "id": "Q28643",
            "label": "paradigm"
          },
          "uses": "Q6015447",
          "P9100": "transfer-learning",
          "openalex_id": "C77075516",
          "schematic": "Transfer learning.svg",
          "different_from": {
            "id": "Q1820378",
            "label": "transfer of learning"
          },
          "mesh_id": "D000098410",
          "mesh_tree_code": [
            "G17.035.250.500.625",
            "L01.224.050.375.530.625"
          ],
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Language model benchmark",
      "entity_type": "data",
      "relation": "uses",
      "relevance": "The paper evaluates AutoPrompt-generated prompts on benchmark datasets to demonstrate the effectiveness of the approach.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model_benchmark",
      "wikidata": {
        "id": "Q135269818",
        "label": "language model benchmark",
        "description": null,
        "aliases": [],
        "claims": {}
      }
    }
  ]
}