{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Reinforcement learning",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "relevance": "RL is the foundational learning paradigm that reward machines enhance by exploiting reward function structure.",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Finite-state machine",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Finite-state_machine",
      "relevance": "Reward machines are based on finite state machines, using their mathematical foundation to structure reward functions.",
      "wikidata": {
        "id": "Q176452",
        "label": "finite-state machine",
        "description": "mathematical model of computation; abstract machine that can be in exactly one of a finite number of states at any given time",
        "aliases": [
          "FSM",
          "finite-state automaton",
          "finite automaton",
          "FA",
          "finite automata",
          "state machine"
        ],
        "claims": {
          "commons_category": "Finite state machine",
          "has_parts": {
            "id": "Q599031",
            "label": "state"
          },
          "freebase_id": "/m/02ykc",
          "stack_exchange_tag": "https://stackoverflow.com/tags/fsm",
          "subclass_of": {
            "id": "Q787114",
            "label": "abstract machine"
          },
          "described_by_source": {
            "id": "Q124355862",
            "label": "Armenian Soviet Encyclopedia, vol. 11"
          },
          "babelnet_id": "00212740n",
          "P5106": [
            "finiteStateMachine",
            "finiteStateAutomaton"
          ],
          "quora_topic_id": "Finite-State-Machines",
          "P1245": "956790",
          "mag_id": "167822520",
          "topic_main_category": {
            "id": "Q25304515",
            "label": "Category:Finite automata"
          },
          "openalex_id": [
            "C167822520",
            "C2983497884"
          ],
          "P691": "ph210246",
          "P6564": "finite-state-machines",
          "P9621": "automa-a-stati-finiti",
          "P9100": "finite-state-machine",
          "opposite_of": {
            "id": "Q137172521",
            "label": "non-finite-state machine"
          }
        }
      }
    },
    {
      "name": "Shaping (psychology)",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Shaping_(psychology)",
      "relevance": "Automated reward shaping is a technique applied to improve learning efficiency by gradually shaping agent behavior.",
      "wikidata": {
        "id": "Q1066177",
        "label": "shaping",
        "description": "psychological paradigm for behavior analysis",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/07scqn",
          "quora_topic_id": "Shaping-1",
          "part_of": {
            "id": "Q77468620",
            "label": "psychology terminology"
          },
          "mag_id": "142311740",
          "openalex_id": "C142311740",
          "instance_of": {
            "id": "Q1799072",
            "label": "method"
          }
        }
      }
    },
    {
      "name": "Hierarchical task network",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Hierarchical_task_network",
      "relevance": "Task decomposition using HTNs enables breaking reward specifications into hierarchically structured learning problems.",
      "wikidata": {
        "id": "Q5753136",
        "label": "Hierarchical task network",
        "description": "approach to automated planning",
        "aliases": [],
        "claims": {
          "mag_id": "129250720",
          "freebase_id": "/m/08l5wk"
        }
      }
    },
    {
      "name": "Counterfactual thinking",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Counterfactual_thinking",
      "relevance": "Counterfactual reasoning is employed to enable off-policy learning by reasoning about alternative action sequences.",
      "wikidata": {
        "id": "Q1783253",
        "label": "counterfactual thinking",
        "description": "concept in psychology that involves the human tendency to create possible alternatives to life events that have already occurred",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/043kt5f",
          "quora_topic_id": "Counterfactual-Thinking",
          "mag_id": "108650721",
          "subclass_of": "Q94626994",
          "P4342": "kontrafaktisk_tenkning",
          "P2579": {
            "id": "Q9418",
            "label": "psychology"
          },
          "has_parts": [
            {
              "id": "Q17949",
              "label": "false statement"
            },
            "Q733541"
          ],
          "openalex_id": "C108650721",
          "openalex_topic_id": "34987"
        }
      }
    },
    {
      "name": "Q-learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Q-learning",
      "relevance": "Off-policy learning algorithms like Q-learning are enhanced through exploitation of reward machine structure.",
      "wikidata": {
        "id": "Q2664563",
        "label": "Q-learning",
        "description": "model-free reinforcement learning algorithm",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/04pvn7",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/q-learning",
            "https://ai.stackexchange.com/tags/q-learning"
          ],
          "quora_topic_id": "Q-learning",
          "P1269": {
            "id": "Q830687",
            "label": "reinforcement learning"
          },
          "mag_id": "188116033",
          "instance_of": {
            "id": "Q8366",
            "label": "algorithm"
          },
          "subclass_of": {
            "id": "Q63788448",
            "label": "model-free reinforcement learning"
          },
          "P2179": "10010329",
          "openalex_id": "C188116033",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Linear temporal logic",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Linear_temporal_logic",
      "relevance": "Reward machines can express temporally extended properties typical of LTL, enabling specification of non-Markovian rewards.",
      "wikidata": {
        "id": "Q1536492",
        "label": "linear temporal logic",
        "description": "field of mathematical logic",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/034swn",
          "commons_category": "Linear temporal logic",
          "quora_topic_id": "Linear-Temporal-Logic",
          "mag_id": "4777664",
          "openalex_id": "C4777664",
          "google_kg_id": "/g/11hgk25c3b",
          "subclass_of": [
            "Q210841",
            "Q781833"
          ],
          "stack_exchange_tag": "https://cs.stackexchange.com/tags/linear-temporal-logic"
        }
      }
    },
    {
      "name": "Regular language",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Regular_language",
      "relevance": "Reward machines have the expressive power of regular languages, defining their theoretical computational boundaries.",
      "wikidata": {
        "id": "Q752532",
        "label": "regular language",
        "description": "formal language that can be expressed using a regular expression",
        "aliases": [
          "rational language"
        ],
        "claims": {
          "freebase_id": "/m/06f10",
          "quora_topic_id": "Regular-Language",
          "subclass_of": "Q729271",
          "P6564": "regular-languages",
          "commons_category": "Regular language",
          "mag_id": "52370388",
          "different_from": {
            "id": "Q3475685",
            "label": "schematic language"
          },
          "openalex_id": "C52370388",
          "P691": "ph237421",
          "P2534": "L\\in\\mathrm{REG}\\Leftrightarrow\\exists A\\in\\mathrm{FSM}:L(A)=L",
          "P7235": [
            "\\mathrm{FSM}",
            "L"
          ],
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "P7726": "RegularLanguage",
          "P9621": "linguaggio-regolare",
          "P7276": "REGUL%C3%81RN%C3%8D%20JAZYK"
        }
      }
    },
    {
      "name": "Reward machines",
      "relation": "proposes",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reward_function",
      "relevance": "The paper proposes reward machines as a novel finite state machine-based formalism for specifying structured reward functions.",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    }
  ]
}