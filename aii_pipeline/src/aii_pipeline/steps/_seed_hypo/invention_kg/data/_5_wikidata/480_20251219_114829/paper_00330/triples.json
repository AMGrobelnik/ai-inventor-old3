{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Perceiver",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "Otter builds upon Flamingo with Perceiver architecture as its foundation for processing multi-modal inputs.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Perceiver",
      "wikidata": {
        "id": "Q108281205",
        "label": "Perceiver",
        "description": "transformer for non-textual data",
        "aliases": [],
        "claims": {
          "subclass_of": "Q85810444",
          "uses": {
            "id": "Q103701642",
            "label": "attention"
          },
          "described_by_source": "Q113364805",
          "P10695": "Q15733006"
        }
      }
    },
    {
      "name": "Fine-tuning (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Instruction tuning is a form of fine-tuning applied to the model for improving instruction following capabilities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
      "wikidata": {
        "id": "Q117286419",
        "label": "fine-tuning",
        "description": "process of taking a pre-trained model and further training it on a smaller, specific dataset to adapt or improve its performance for a particular task or domain",
        "aliases": [
          "neural network fine-tuning"
        ],
        "claims": {
          "instance_of": "Q117348143",
          "commons_category": "AI model fine-tuning",
          "follows": {
            "id": "Q124149651",
            "label": "pre-training"
          },
          "subclass_of": "Q1714153",
          "P10": "Easiest Way to Build Consistent Character - Step-by-Step Guide with OpenArt.webm"
        }
      }
    },
    {
      "name": "In-context learning (natural language processing)",
      "entity_type": "concept",
      "relation": "proposes",
      "relevance": "Otter is specifically designed to leverage both textual and visual in-context examples to enhance instruction following.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/In-context_learning_(natural_language_processing)",
      "wikidata": {
        "id": "Q108941486",
        "label": "prompt engineering",
        "description": "creation or optimization of a prompt to be given to an artificial intelligence model",
        "aliases": [
          "prompt-based learning",
          "AI prompt engineering"
        ],
        "claims": {
          "subclass_of": [
            "Q110484020",
            {
              "id": "Q80006",
              "label": "computer programming"
            }
          ],
          "instance_of": [
            {
              "id": "Q2695280",
              "label": "technique"
            },
            {
              "id": "Q627436",
              "label": "field of work"
            }
          ],
          "P1269": {
            "id": "Q11660",
            "label": "artificial intelligence"
          },
          "use": [
            "Q65066631",
            "Q117246174"
          ],
          "P1056": {
            "id": "Q117217619",
            "label": "AI prompt"
          },
          "P5008": "Q117245199",
          "practiced_by": [
            "Q117674392",
            "Q123653520"
          ],
          "commons_category": "Prompt engineering for generative AI",
          "google_kg_id": "/g/11p6kpgt_n",
          "P691": "ph1266210"
        }
      }
    },
    {
      "name": "Multimodal learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Otter is a multi-modal model that processes text, images, and video as integrated modalities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multimodal_learning",
      "wikidata": {
        "id": "Q25052564",
        "label": "multimodal learning",
        "description": "machine learning combining different information resources, such as images and text",
        "aliases": [],
        "claims": {
          "subclass_of": "Q2539",
          "mag_id": "2780660688",
          "freebase_id": "/m/013f3fbp",
          "openalex_id": "C2780660688",
          "commons_category": "Multimodal machine learning",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Large language model",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Otter extends large multimodal models with instruction tuning capabilities for general-purpose assistance.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "wikidata": {
        "id": "Q115305900",
        "label": "large language model",
        "description": "language model built with very large amounts of texts",
        "aliases": [
          "LLM",
          "LLMs",
          "large language models",
          "word soup machine",
          "word soup model",
          "word salad machine",
          "word salad model"
        ],
        "claims": {
          "subclass_of": "Q3621696",
          "P1269": {
            "id": "Q11660",
            "label": "artificial intelligence"
          },
          "short_name": "LLM",
          "uses": [
            {
              "id": "Q117217619",
              "label": "AI prompt"
            },
            "Q85810444"
          ],
          "P5008": "Q117245199",
          "topic_main_category": {
            "id": "Q117280639",
            "label": "Category:Large language models"
          },
          "has_parts": [
            {
              "id": "Q116777014",
              "label": "generative pre-trained transformer"
            },
            "Q117246174",
            "Q3614994"
          ],
          "P973": "https://www.youtube.com/watch?v=WqYBx2gB6vA",
          "P5844": "neo-modello-linguistico-di-grandi-dimensioni_(Neologismi)",
          "opposite_of": "Q123759530",
          "google_kg_id": "/g/11kc9956b3",
          "gnd_id": "1322631905",
          "P12861": "E434",
          "use": [
            "Q116961403",
            {
              "id": "Q3510521",
              "label": "computer security"
            }
          ],
          "instance_of": {
            "id": "Q117349475",
            "label": "artificial intelligence model type"
          },
          "commons_category": "Large language models",
          "britannica_id": "topic/large-language-model",
          "P989": [
            "Wikipedia - Large language model (spoken by AI voice).mp3",
            "Wikipedia - Modelo extenso de lenguaje (hablado por voz AI).mp3"
          ],
          "mesh_id": "D000098342",
          "mesh_tree_code": [
            "G17.035.250.500.250.500",
            "G17.485.500.500",
            "L01.224.050.375.530.250.500",
            "L01.224.050.375.605.500.500",
            "L01.224.494"
          ],
          "P3911": "30478-6",
          "P2354": {
            "id": "Q131598147",
            "label": "list of large language models"
          },
          "P1368": "000355290",
          "described_by_source": "Q133280509",
          "P508": "77644",
          "P13572": "large+language+model",
          "stack_exchange_tag": [
            "https://genai.stackexchange.com/tags/llm",
            "https://ai.stackexchange.com/tags/large-language-models",
            "https://stats.stackexchange.com/tags/llm",
            "https://stackoverflow.com/tags/large-language-model",
            "https://datascience.stackexchange.com/tags/llm",
            "https://softwarerecs.stackexchange.com/tags/llm",
            "https://mathematica.stackexchange.com/tags/llm-functions",
            "https://or.stackexchange.com/tags/llm",
            "https://law.stackexchange.com/tags/llm",
            "https://hermeneutics.stackexchange.com/tags/llm"
          ],
          "P9100": "llm",
          "P691": "ph1274563"
        }
      }
    },
    {
      "name": "Computer vision",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Otter handles visual understanding tasks including image and video processing as core capabilities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Computer_vision",
      "wikidata": {
        "id": "Q844240",
        "label": "computer vision",
        "description": "computerized information extraction from images",
        "aliases": [],
        "claims": {
          "topic_main_category": "Q5875502",
          "freebase_id": "/m/01xzx",
          "stack_exchange_tag": "https://stackoverflow.com/tags/computer-vision",
          "commons_category": "Computer vision",
          "P1051": "13621",
          "P3222": "datorseende",
          "quora_topic_id": "Computer-Vision",
          "P3827": "image-classification",
          "same_as": {
            "id": "Q1425977",
            "label": "machine vision"
          },
          "britannica_id": "technology/computer-vision",
          "P4746": "137581",
          "loc_id": "sh85029549",
          "P5922": "080104",
          "mag_id": "31972630",
          "P508": "61074",
          "instance_of": "Q11862829",
          "P8408": "ComputerVision",
          "P8529": "460304",
          "P3984": "computervision",
          "P3847": "computer_vision",
          "P9100": "computer-vision",
          "P9545": "210476",
          "P268": "11976826n",
          "P3553": "19590195",
          "P2004": "203396",
          "P8189": "987007545617805171",
          "openalex_id": "C31972630",
          "P4073": "computervision",
          "P2179": "10010224",
          "P3235": "computer-vision",
          "P6009": "32309",
          "P10376": "computer-science/computer-vision",
          "P691": "ph344057",
          "openalex_topic_id": "196872",
          "part_of": {
            "id": "Q113212481",
            "label": "computer vision and multimedia computation"
          },
          "P7502": "Computer_Vision-RN8",
          "practiced_by": {
            "id": "Q116952811",
            "label": "computer vision engineer"
          },
          "P10022": "visione-artificiale",
          "P12946": "computer+vision",
          "P13572": "computer+vision",
          "P13591": "concept/f2822c65-7d89-4cf3-bfbd-096866210fa3",
          "P13397": "computer+vision"
        }
      }
    },
    {
      "name": "Transformer (deep learning)",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "The transformer architecture underpins the Perceiver architecture used in Otter for processing multi-modal data.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
      "wikidata": {
        "id": "Q85810444",
        "label": "transformer",
        "description": "machine-learning model architecture first developed by Google Brain",
        "aliases": [
          "transformer model",
          "transformer architecture",
          "transformers"
        ],
        "claims": {
          "subclass_of": [
            "Q192776",
            {
              "id": "Q113364611",
              "label": "deep learning model"
            }
          ],
          "developer": [
            {
              "id": "Q16927616",
              "label": "Google Brain"
            },
            {
              "id": "Q44749723",
              "label": "Ashish Vaswani"
            },
            "Q30251943"
          ],
          "follows": [
            {
              "id": "Q1457734",
              "label": "recurrent neural network"
            },
            {
              "id": "Q6673524",
              "label": "long short-term memory"
            }
          ],
          "described_by_source": {
            "id": "Q30249683",
            "label": "Attention Is All You Need"
          },
          "google_kg_id": "/g/11hz_m4ssw",
          "has_parts": [
            "Q42586063",
            {
              "id": "Q745243",
              "label": "decoder"
            }
          ],
          "uses": [
            {
              "id": "Q103701642",
              "label": "attention"
            },
            {
              "id": "Q5441227",
              "label": "feedforward neural network"
            }
          ],
          "use": [
            {
              "id": "Q30642",
              "label": "natural language processing"
            },
            {
              "id": "Q844240",
              "label": "computer vision"
            },
            {
              "id": "Q3245113",
              "label": "statistical machine translation"
            },
            {
              "id": "Q1394144",
              "label": "automatic summarization"
            }
          ],
          "schematic": "Transformer, full architecture.png",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/transformer",
          "P575": "2017-06-12",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Attention (machine learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Attention mechanisms are fundamental to both the Perceiver architecture and the transformer-based design of Otter.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
      "wikidata": {
        "id": "Q103701642",
        "label": "attention",
        "description": "machine learning technique",
        "aliases": [
          "attention mechanism"
        ],
        "claims": {
          "subclass_of": "Q6501338",
          "P1269": "Q2539",
          "google_kg_id": "/g/11qpclnpwr",
          "use": [
            "Q192776",
            {
              "id": "Q25325414",
              "label": "neural Turing machine"
            },
            {
              "id": "Q28324912",
              "label": "differentiable neural computer"
            },
            "Q85810444",
            "Q108281205"
          ],
          "P1557": "Q6501338",
          "openalex_topic_id": "413584",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/attention",
          "schematic": "Attention mechanism overview.svg"
        }
      }
    }
  ]
}