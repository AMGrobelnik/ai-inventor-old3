{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Policy gradient method",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "The paper proposes novel theoretical guarantees for policy gradient methods showing global optimality under specific MDP structural conditions.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Policy_gradient_method",
      "wikidata": {
        "id": "Q113840014",
        "label": "policy-gradient method",
        "description": "class of reinforcement learning algorithms",
        "aliases": [
          "policy gradient method",
          "policy gradient"
        ],
        "claims": {
          "subclass_of": {
            "id": "Q830687",
            "label": "reinforcement learning"
          },
          "P9526": "Policy_gradient_methods",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Reinforcement learning",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Policy gradient methods are a core technique in reinforcement learning that the paper provides theoretical analysis for.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Stochastic gradient descent",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Policy gradient methods perform stochastic gradient descent on the cumulative expected cost-to-go, which is the fundamental optimization technique analyzed.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent",
      "wikidata": {
        "id": "Q7617819",
        "label": "stochastic gradient descent",
        "description": "gradient descent method used for the minimization of an objective function",
        "aliases": [
          "SGD",
          "SGD optimizer"
        ],
        "claims": {
          "subclass_of": "Q1199743",
          "P1552": {
            "id": "Q1071239",
            "label": "stochastic"
          },
          "P7502": "Stochastic_gradient_descent_(SGD)",
          "freebase_id": "/m/04f0gg",
          "P9100": "stochastic-gradient-descent",
          "instance_of": {
            "id": "Q24034552",
            "label": "mathematical concept"
          },
          "quora_topic_id": "Stochastic-gradient-descent",
          "use": "Q2539",
          "short_name": "SGD",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Markov decision process",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "MDPs provide the mathematical framework for the control problems analyzed; the paper identifies structural properties of MDPs that guarantee global optimality.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "wikidata": {
        "id": "Q176789",
        "label": "Markov decision process",
        "description": "mathematical model for sequential decision making under uncertainty",
        "aliases": [
          "MDP",
          "MDPs"
        ],
        "claims": {
          "P138": "Q176659",
          "freebase_id": "/m/048gl8",
          "P1051": "7713",
          "P4969": "Q176814",
          "mag_id": "106189395",
          "openalex_id": "C106189395",
          "subclass_of": [
            "Q1331926",
            {
              "id": "Q176737",
              "label": "stochastic process"
            }
          ],
          "openalex_topic_id": "56113",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Policy iteration",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "The paper establishes a key connection between policy iteration (a dynamic programming algorithm) and policy gradient methods to derive its main theoretical results.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "wikidata": {
        "id": "Q176789",
        "label": "Markov decision process",
        "description": "mathematical model for sequential decision making under uncertainty",
        "aliases": [
          "MDP",
          "MDPs"
        ],
        "claims": {
          "P138": "Q176659",
          "freebase_id": "/m/048gl8",
          "P1051": "7713",
          "P4969": "Q176814",
          "mag_id": "106189395",
          "openalex_id": "C106189395",
          "subclass_of": [
            "Q1331926",
            {
              "id": "Q176737",
              "label": "stochastic process"
            }
          ],
          "openalex_topic_id": "56113",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Łojasiewicz inequality",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The Polyak-Lojasiewicz (gradient dominance) condition is shown to be satisfied under certain conditions, yielding fast convergence rates.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/%C5%81ojasiewicz_inequality",
      "wikidata": {
        "id": "Q8080668",
        "label": "Łojasiewicz inequality",
        "description": "inequality from distance to a zero of a real analytic function",
        "aliases": [],
        "claims": {
          "instance_of": "Q28113351",
          "P138": "Q1800585",
          "P2534": "\\operatorname{dist}(x,Z)^\\alpha \\le C|f(x)|",
          "freebase_id": "/m/05zhkdd",
          "P2579": {
            "id": "Q149972",
            "label": "calculus"
          },
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          }
        }
      }
    },
    {
      "name": "LQR",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Linear quadratic control is one of the classical control problems where the authors instantiate and validate their theoretical framework.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/LQR",
      "wikidata": {
        "id": "Q3820457",
        "label": "LQR",
        "description": "Wikimedia disambiguation page",
        "aliases": [],
        "claims": {
          "instance_of": {
            "id": "Q4167410",
            "label": "Wikimedia disambiguation page"
          }
        }
      }
    },
    {
      "name": "Optimal stopping",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Optimal stopping is a classical control problem used as an application domain to validate the paper's theoretical framework.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Optimal_stopping",
      "wikidata": {
        "id": "Q7098950",
        "label": "optimal stopping",
        "description": "class of mathematical problems concerned with choosing an optimal time to take a particular action",
        "aliases": [
          "optimal stopping problem",
          "early stopping"
        ],
        "claims": {
          "P2534": "V_t^T = \\mathbb{E} G_{\\tau^*} = \\sup_{t\\le \\tau \\le T} \\mathbb{E} G_\\tau",
          "loc_id": "sh85095192",
          "mag_id": "99414536",
          "freebase_id": "/m/025vlt8",
          "subclass_of": "Q984063",
          "openalex_id": "C99414536",
          "P8189": "987007548409605171",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "P13591": "concept/90effa39-fcc4-4ff5-96a4-fe9fd013d403"
        }
      }
    }
  ]
}