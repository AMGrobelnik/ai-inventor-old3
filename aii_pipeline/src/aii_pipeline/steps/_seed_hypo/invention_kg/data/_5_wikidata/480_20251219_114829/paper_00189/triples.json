{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Monte Carlo method",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "The paper proposes a Monte-Carlo simulation algorithm as the core technique for real-time policy improvement.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Monte_Carlo_method",
      "wikidata": {
        "id": "Q232207",
        "label": "Monte Carlo method",
        "description": "broad class of computational algorithms using random sampling to obtain numerical results",
        "aliases": [
          "MC method",
          "Monte Carlo experiment",
          "Monte Carlo simulation",
          "Monte Carlo algorithm"
        ],
        "claims": {
          "commons_category": "Monte Carlo method",
          "P508": "32258",
          "topic_main_category": "Q9210903",
          "P349": "00567842",
          "freebase_id": "/m/0fjvb",
          "subclass_of": [
            {
              "id": "Q583461",
              "label": "randomized algorithm"
            },
            "Q45045"
          ],
          "P1051": "7198",
          "P138": {
            "id": "Q1779905",
            "label": "Monte Carlo Casino"
          },
          "P61": [
            "Q8753",
            "Q234357",
            "Q17455"
          ],
          "gnd_id": "4240945-7",
          "instance_of": {
            "id": "Q583461",
            "label": "randomized algorithm"
          },
          "britannica_id": "science/Monte-Carlo-method",
          "quora_topic_id": "Monte-Carlo-Techniques",
          "P3827": "monte-carlo-methods",
          "mesh_id": "D009010",
          "P2812": "MonteCarloMethod",
          "loc_id": "sh85087032",
          "P2924": "2228629",
          "described_by_source": {
            "id": "Q124737635",
            "label": "Armenian Soviet Encyclopedia, vol. 8"
          },
          "P1296": "0043727",
          "mesh_tree_code": [
            "E05.318.740.525",
            "L01.906.394.422",
            "N05.715.360.750.540",
            "N06.850.520.830.525"
          ],
          "P4732": "MT07072",
          "mag_id": "19499675",
          "P2347": "6361",
          "different_from": "Q15238499",
          "P8408": "MonteCarloMethod",
          "P9100": "monte-carlo-simulation",
          "image": "Pi 30K.gif",
          "P8189": "987007543534905171",
          "openalex_id": "C19499675",
          "P8313": "Monte_Carlo-metoder",
          "P4342": "Monte_Carlo-metode",
          "P691": "ph122780",
          "openalex_topic_id": [
            "76305",
            "321898",
            "234677",
            "154023"
          ],
          "P3911": "15267-6",
          "stack_exchange_tag": [
            "https://ai.stackexchange.com/tags/monte-carlo-methods",
            "https://computergraphics.stackexchange.com/tags/monte-carlo"
          ],
          "P3222": "monte-carlo-metoder",
          "P2892": "C0026507",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "P6564": "monte-carlo",
          "P12385": "metode-de-montecarlo",
          "P1036": "518.282",
          "P13591": "concept/be660e7d-7744-46b7-9240-80a1964be733"
        }
      }
    },
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper builds on reinforcement learning principles for policy improvement using reward maximization.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Backgammon",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Backgammon is the primary application domain where the algorithm is experimentally evaluated.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Backgammon",
      "wikidata": {
        "id": "Q11411",
        "label": "backgammon",
        "description": "one of the oldest board games for two players",
        "aliases": [],
        "claims": {
          "commons_category": "Backgammon",
          "P508": "16757",
          "gnd_id": "4069063-5",
          "topic_main_category": "Q8285557",
          "P349": "00577370",
          "freebase_id": "/m/01d30",
          "image": "Backgammon lg.jpg",
          "P2339": "2397",
          "subclass_of": "Q1188693",
          "quora_topic_id": "Backgammon",
          "P1014": "300222747",
          "P3827": "backgammon",
          "P3222": "backgammon",
          "P2812": "Backgammon",
          "P3219": "backgammon",
          "babelnet_id": "00007779n",
          "practiced_by": {
            "id": "Q23929009",
            "label": "backgammon player"
          },
          "P5008": [
            "Q5460604",
            {
              "id": "Q6173448",
              "label": "Wikipedia:Vital articles/Level/4"
            }
          ],
          "britannica_id": "topic/backgammon",
          "instance_of": [
            "Q131436",
            "Q1515156",
            {
              "id": "Q17351672",
              "label": "social game"
            },
            {
              "id": "Q47728",
              "label": "hobby"
            }
          ],
          "described_by_source": [
            {
              "id": "Q602358",
              "label": "Brockhaus and Efron Encyclopedic Dictionary"
            },
            "Q19219752",
            {
              "id": "Q867541",
              "label": "Encyclopædia Britannica 11th edition"
            }
          ],
          "P989": "Backgammon.ogg",
          "P1245": "858195",
          "P8408": "Backgammon",
          "P8519": "66369",
          "P4342": "backgammon",
          "use": [
            {
              "id": "Q173799",
              "label": "entertainment"
            },
            "Q349",
            {
              "id": "Q841654",
              "label": "competition"
            }
          ],
          "loc_id": "sh85010798",
          "P1872": "+2",
          "P1873": "+2",
          "P2899": "+5",
          "P8514": "gamao  vhdbj cgpñbebewoakn n gjkeola,sbx ,cvmb",
          "P268": "11938367h",
          "P8189": "987007284650805171",
          "P8313": "backgammon",
          "P8814": "00503833-n",
          "P7749": "14218",
          "P5160": "tgm000705",
          "coincident_with": {
            "id": "Q968853",
            "label": "Tavli"
          },
          "P3984": "backgammon",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "P2924": "2249090",
          "P11137": "backgammon",
          "P12596": "21436",
          "P8309": "18-164161",
          "P13397": "Backgammon",
          "P13591": "concept/536912f4-3185-4272-b056-44ac70e0f680",
          "P1036": "795.15"
        }
      }
    },
    {
      "name": "Multilayer perceptron",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "The baseline TD-Gammon uses multilayer neural networks, which the Monte-Carlo algorithm improves upon.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multilayer_perceptron",
      "wikidata": {
        "id": "Q2991667",
        "label": "multilayer perceptron",
        "description": "type of feedforward neural network with multiple fully connected layers",
        "aliases": [
          "MLP",
          "multi-layer perceptron"
        ],
        "claims": {
          "freebase_id": "/m/03bx6t8",
          "subclass_of": {
            "id": "Q5441227",
            "label": "feedforward neural network"
          },
          "P144": {
            "id": "Q690207",
            "label": "perceptron"
          },
          "mag_id": "179717631",
          "P61": "Q93018",
          "openalex_id": "C179717631",
          "openalex_topic_id": [
            "203778",
            "375940",
            "509954"
          ],
          "uses": [
            {
              "id": "Q798503",
              "label": "backpropagation"
            },
            "Q4677469"
          ],
          "use": [
            {
              "id": "Q16250539",
              "label": "fitness approximation"
            },
            "Q1744628",
            {
              "id": "Q16346",
              "label": "speech synthesis"
            },
            {
              "id": "Q172491",
              "label": "data mining"
            }
          ],
          "P7502": "Multilayer_perceptron-Y3RWAW",
          "mesh_id": "D000098421",
          "mesh_tree_code": [
            "G17.485.625.750",
            "L01.224.050.375.605.625.750"
          ]
        }
      }
    },
    {
      "name": "TD-Gammon",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "TD-Gammon is a strong baseline neural network player that the proposed algorithm is compared against.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/TD-Gammon",
      "wikidata": {
        "id": "Q7669824",
        "label": "TD-Gammon",
        "description": "is a computer backgammon program developed in 1992",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/0b6m8hy",
          "P9100": "td-gammon"
        }
      }
    },
    {
      "name": "Temporal difference learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Temporal difference learning is the underlying method used by TD-Gammon, serving as a comparison baseline.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Temporal_difference_learning",
      "wikidata": {
        "id": "Q7698910",
        "label": "Temporal difference learning",
        "description": "intelligent Tutoring System",
        "aliases": [
          "TD Models"
        ],
        "claims": {
          "mag_id": "196340769",
          "freebase_id": "/m/04hhdt",
          "uses": "Q2219080",
          "openalex_id": "C196340769",
          "openalex_topic_id": "390701",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    }
  ]
}