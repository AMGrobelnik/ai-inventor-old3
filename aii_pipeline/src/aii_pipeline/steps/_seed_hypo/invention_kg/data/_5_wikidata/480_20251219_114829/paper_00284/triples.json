{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "BERT (language model)",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
      "relevance": "The paper builds upon BERT as a pre-trained transformer model to enable multimodal adaptation.",
      "wikidata": {
        "id": "Q61726893",
        "label": "bidirectional encoder representations from transformers",
        "description": "deep learning artificial neural network language model",
        "aliases": [
          "BERT",
          "bidirectional encoder representations from transformer"
        ],
        "claims": {
          "described_by_source": "Q57267388",
          "P571": "2018-00-00",
          "P3575": [
            "+110000000",
            "+340000000"
          ],
          "instance_of": [
            {
              "id": "Q115305900",
              "label": "large language model"
            },
            "Q85810444",
            "Q24868018"
          ],
          "developer": {
            "id": "Q28943742",
            "label": "Google Research"
          },
          "P1324": "https://github.com/google-research/bert",
          "P275": {
            "id": "Q13785927",
            "label": "Apache Software License 2.0"
          },
          "use": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "google_kg_id": "/g/11h75tkhxs",
          "P9100": "bert",
          "P138": {
            "id": "Q584184",
            "label": "Bert"
          },
          "P6216": {
            "id": "Q50423863",
            "label": "copyrighted"
          },
          "P973": [
            "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html",
            "https://devopedia.org/bert-language-model"
          ],
          "subclass_of": "Q85810444",
          "P856": "https://arxiv.org/abs/1810.04805",
          "commons_category": "BERT"
        }
      }
    },
    {
      "name": "XLNet",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/XLNet",
      "relevance": "The paper adapts XLNet with the MAG framework to handle multimodal inputs for sentiment analysis.",
      "wikidata": {
        "id": "Q64732735",
        "label": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "description": "scientific article published on 19 June 2019",
        "aliases": [],
        "claims": {
          "P818": "1906.08237",
          "instance_of": {
            "id": "Q13442814",
            "label": "scholarly article"
          },
          "P1476": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "P577": "2019-06-19",
          "P820": [
            "cs.CL",
            "cs.LG"
          ],
          "P2093": [
            "Zhilin Yang",
            "Zihang Dai",
            "Yiming Yang"
          ],
          "main_subject": [
            "Q1078276",
            {
              "id": "Q197536",
              "label": "deep learning"
            },
            {
              "id": "Q107031747",
              "label": "XLNet"
            }
          ],
          "P4510": "Q64711428",
          "P50": [
            {
              "id": "Q29043123",
              "label": "Quoc Viet Le"
            },
            {
              "id": "Q6123694",
              "label": "Jaime Carbonell"
            },
            {
              "id": "Q28016131",
              "label": "Ruslan Salakhutdinov"
            }
          ],
          "P407": "Q1860",
          "P1104": "+18",
          "P2860": {
            "id": "Q64711427",
            "label": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
          },
          "P1433": "Q118398"
        }
      }
    },
    {
      "name": "Transformer (deep learning)",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
      "relevance": "Transformers are the foundational architecture that BERT and XLNet are built upon.",
      "wikidata": {
        "id": "Q85810444",
        "label": "transformer",
        "description": "machine-learning model architecture first developed by Google Brain",
        "aliases": [
          "transformer model",
          "transformer architecture",
          "transformers"
        ],
        "claims": {
          "subclass_of": [
            "Q192776",
            {
              "id": "Q113364611",
              "label": "deep learning model"
            }
          ],
          "developer": [
            {
              "id": "Q16927616",
              "label": "Google Brain"
            },
            {
              "id": "Q44749723",
              "label": "Ashish Vaswani"
            },
            "Q30251943"
          ],
          "follows": [
            {
              "id": "Q1457734",
              "label": "recurrent neural network"
            },
            {
              "id": "Q6673524",
              "label": "long short-term memory"
            }
          ],
          "described_by_source": {
            "id": "Q30249683",
            "label": "Attention Is All You Need"
          },
          "google_kg_id": "/g/11hz_m4ssw",
          "has_parts": [
            "Q42586063",
            {
              "id": "Q745243",
              "label": "decoder"
            }
          ],
          "uses": [
            {
              "id": "Q103701642",
              "label": "attention"
            },
            {
              "id": "Q5441227",
              "label": "feedforward neural network"
            }
          ],
          "use": [
            {
              "id": "Q30642",
              "label": "natural language processing"
            },
            {
              "id": "Q844240",
              "label": "computer vision"
            },
            {
              "id": "Q3245113",
              "label": "statistical machine translation"
            },
            {
              "id": "Q1394144",
              "label": "automatic summarization"
            }
          ],
          "schematic": "Transformer, full architecture.png",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/transformer",
          "P575": "2017-06-12",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Natural language processing",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
      "relevance": "The paper addresses multimodal sentiment analysis, a task within the NLP domain.",
      "wikidata": {
        "id": "Q30642",
        "label": "natural language processing",
        "description": "field of computer science and linguistics",
        "aliases": [
          "NLP"
        ],
        "claims": {
          "P349": "00562347",
          "topic_main_category": {
            "id": "Q9149760",
            "label": "Category:Natural language processing"
          },
          "P1051": "12529",
          "commons_category": "Natural language processing",
          "subclass_of": [
            {
              "id": "Q11660",
              "label": "artificial intelligence"
            },
            "Q21198",
            "Q182557",
            "Q11862829",
            "Q750843"
          ],
          "mesh_id": "D009323",
          "P3827": "natural-language-processing",
          "study_of": [
            {
              "id": "Q2554325",
              "label": "lemmatisation"
            },
            {
              "id": "Q1271424",
              "label": "part-of-speech tagging"
            },
            "Q194152",
            {
              "id": "Q7451191",
              "label": "sentence boundary disambiguation"
            },
            {
              "id": "Q1416732",
              "label": "stemming"
            },
            {
              "id": "Q2748079",
              "label": "terminology extraction"
            },
            {
              "id": "Q1759657",
              "label": "lexical semantics"
            },
            "Q79798",
            "Q403574",
            {
              "id": "Q1513879",
              "label": "natural language generation"
            },
            "Q167555",
            "Q1074173",
            "Q6588467",
            {
              "id": "Q7310755",
              "label": "relationship extraction"
            },
            {
              "id": "Q2271421",
              "label": "sentiment analysis"
            },
            "Q1948408",
            "Q48522",
            {
              "id": "Q1394144",
              "label": "automatic summarization"
            },
            "Q63087",
            {
              "id": "Q1129466",
              "label": "discourse analysis"
            },
            {
              "id": "Q189436",
              "label": "speech recognition"
            },
            {
              "id": "Q2266173",
              "label": "speech segmentation"
            },
            {
              "id": "Q16346",
              "label": "speech synthesis"
            },
            "Q18395344",
            {
              "id": "Q47165059",
              "label": "decompounding"
            },
            {
              "id": "Q2438971",
              "label": "tokenization"
            }
          ],
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/nlp",
            "https://ai.stackexchange.com/tags/natural-language-processing"
          ],
          "britannica_id": "technology/natural-language-processing-computer-science",
          "P3219": "traitement-automatique-des-langues",
          "loc_id": "sh88002425",
          "P3235": "natural-language-processing",
          "P5922": "080107",
          "P1296": "0281325",
          "short_name": [
            "NLP",
            "TAL",
            "TALN",
            "PLN",
            "PLN",
            "PLN",
            "NLP",
            "PIN"
          ],
          "P7502": "Natural_language_processing_(NLP)-DZY",
          "P3984": "LanguageTechnology",
          "topic_maintained_by": {
            "id": "Q23006254",
            "label": "Template:Natural language processing"
          },
          "P9100": [
            "natural-language-processing",
            "nlp"
          ],
          "freebase_id": "/m/05flf",
          "P9545": "249047",
          "P3553": "19560026",
          "P443": "LL-Q150 (fra)-Visiteur Journ√©e 2 - 18 (Madehub)-traitement automatique du langage naturel.wav",
          "P8189": "987007536703305171",
          "quora_topic_id": "Natural-Language-Processing",
          "P8529": "460208",
          "openalex_id": "C204321447",
          "P4644": "fff0e2cd-d0bd-4b02-9daf-158b79d9688a",
          "mesh_tree_code": "L01.224.050.375.580",
          "openalex_topic_id": "91728",
          "P691": "ph427562",
          "P3847": [
            "natural_language_processing_(computer_science)",
            "natural_language_processing"
          ],
          "P2572": "NLProc",
          "practiced_by": {
            "id": "Q116952787",
            "label": "natural language processing engineer"
          },
          "P2892": "C0027489",
          "P9309": "naturalLanguageProcessing",
          "instance_of": [
            "Q11862829",
            {
              "id": "Q1047113",
              "label": "field of study"
            },
            "Q2267705",
            {
              "id": "Q268592",
              "label": "industry"
            },
            {
              "id": "Q135892289",
              "label": "branch of linguistics"
            }
          ],
          "P5844": "neo-elaborazione-del-linguaggio-naturale_(Neologismi)",
          "P12385": "processament-del-llenguatge-natural",
          "P5437": "c_67092197",
          "image": "T-SNE visualisation of word embeddings generated using 19th century literature.png",
          "P13397": "NLP",
          "P13591": "concept/e0fc4a1c-7602-43f9-9cea-0d8bb49039b8"
        }
      }
    },
    {
      "name": "Sentiment analysis",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Sentiment_analysis",
      "relevance": "The paper focuses on improving multimodal sentiment analysis performance on benchmark datasets.",
      "wikidata": {
        "id": "Q2271421",
        "label": "sentiment analysis",
        "description": "use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials",
        "aliases": [
          "opinion mining",
          "Opinion Extraction"
        ],
        "claims": {
          "freebase_id": "/m/0g57xn",
          "quora_topic_id": "Sentiment-Analysis",
          "P3827": "sentiment-analysis",
          "subclass_of": [
            "Q1185804",
            {
              "id": "Q30642",
              "label": "natural language processing"
            }
          ],
          "commons_category": "Sentiment analysis",
          "P2179": "10003353",
          "P2579": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "use": "Q39809",
          "instance_of": {
            "id": "Q627436",
            "label": "field of work"
          },
          "mag_id": "66402592",
          "stack_exchange_tag": [
            "https://datascience.stackexchange.com/tags/sentiment-analysis",
            "https://stackoverflow.com/tags/sentiment-analysis"
          ],
          "uses": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "P3553": "19966010",
          "P9100": "sentiment-analysis",
          "openalex_id": "C66402592",
          "mesh_id": "D000090042",
          "mesh_tree_code": [
            "G17.035.250.750",
            "L01.224.050.375.815",
            "L01.313.500.750.280.199.750",
            "L01.470.625.750"
          ],
          "openalex_topic_id": "219676",
          "P7502": "Sentiment_analysis-V48YAR",
          "P2892": "C5544431",
          "P9309": "sentimentAnalysis",
          "P8168": "Q1194520",
          "P13397": "sentiment+analysis"
        }
      }
    },
    {
      "name": "Multimodal sentiment analysis",
      "relation": "proposes",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis",
      "relevance": "The paper advances multimodal sentiment analysis by enabling transformers to process visual and acoustic modalities.",
      "wikidata": {
        "id": "Q55008106",
        "label": "Multimodal sentiment analysis",
        "description": "technology for sentiment analysis",
        "aliases": [],
        "claims": {
          "subclass_of": {
            "id": "Q2271421",
            "label": "sentiment analysis"
          },
          "google_kg_id": "/g/11f6cmdc2s",
          "P1269": {
            "id": "Q11660",
            "label": "artificial intelligence"
          }
        }
      }
    },
    {
      "name": "Multimodal learning",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multimodal_learning",
      "relevance": "The paper integrates vision and acoustic modalities with language for improved sentiment understanding.",
      "wikidata": {
        "id": "Q25052564",
        "label": "multimodal learning",
        "description": "machine learning combining different information resources, such as images and text",
        "aliases": [],
        "claims": {
          "subclass_of": "Q2539",
          "mag_id": "2780660688",
          "freebase_id": "/m/013f3fbp",
          "openalex_id": "C2780660688",
          "commons_category": "Multimodal machine learning",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Fine-tuning (deep learning)",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
      "relevance": "The paper applies fine-tuning to pre-trained BERT and XLNet models on multimodal datasets.",
      "wikidata": {
        "id": "Q117286419",
        "label": "fine-tuning",
        "description": "process of taking a pre-trained model and further training it on a smaller, specific dataset to adapt or improve its performance for a particular task or domain",
        "aliases": [
          "neural network fine-tuning"
        ],
        "claims": {
          "instance_of": "Q117348143",
          "commons_category": "AI model fine-tuning",
          "follows": {
            "id": "Q124149651",
            "label": "pre-training"
          },
          "subclass_of": "Q1714153",
          "P10": "Easiest Way to Build Consistent Character - Step-by-Step Guide with OpenArt.webm"
        }
      }
    },
    {
      "name": "Attention (machine learning)",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
      "relevance": "Attention mechanisms are core to the transformer architecture that the paper extends with multimodal capabilities.",
      "wikidata": {
        "id": "Q103701642",
        "label": "attention",
        "description": "machine learning technique",
        "aliases": [
          "attention mechanism"
        ],
        "claims": {
          "subclass_of": "Q6501338",
          "P1269": "Q2539",
          "google_kg_id": "/g/11qpclnpwr",
          "use": [
            "Q192776",
            {
              "id": "Q25325414",
              "label": "neural Turing machine"
            },
            {
              "id": "Q28324912",
              "label": "differentiable neural computer"
            },
            "Q85810444",
            "Q108281205"
          ],
          "P1557": "Q6501338",
          "openalex_topic_id": "413584",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/attention",
          "schematic": "Attention mechanism overview.svg"
        }
      }
    }
  ]
}