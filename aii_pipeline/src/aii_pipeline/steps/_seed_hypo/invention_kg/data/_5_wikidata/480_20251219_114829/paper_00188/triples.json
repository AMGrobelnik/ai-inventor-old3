{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper builds on reinforcement learning concepts, specifically adapting them to the inverse problem of learning reward functions.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Markov decision process",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper addresses reward learning in MDPs, which is the fundamental framework for the inverse reinforcement learning problem.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "wikidata": {
        "id": "Q176789",
        "label": "Markov decision process",
        "description": "mathematical model for sequential decision making under uncertainty",
        "aliases": [
          "MDP",
          "MDPs"
        ],
        "claims": {
          "P138": "Q176659",
          "freebase_id": "/m/048gl8",
          "P1051": "7713",
          "P4969": "Q176814",
          "mag_id": "106189395",
          "openalex_id": "C106189395",
          "subclass_of": [
            "Q1331926",
            {
              "id": "Q176737",
              "label": "stochastic process"
            }
          ],
          "openalex_topic_id": "56113",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Bayesian inference",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "The core contribution of the paper is combining Bayesian inference with inverse reinforcement learning to create probability distributions over reward functions.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Bayesian_inference",
      "wikidata": {
        "id": "Q812535",
        "label": "Bayesian inference",
        "description": "method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available",
        "aliases": [
          "Bayesian analysis",
          "Bayes' solution",
          "Bayesian approach",
          "Bayesian method",
          "Bayes inference",
          "Bayesian updating"
        ],
        "claims": {
          "gnd_id": "4144220-9",
          "P508": "36249",
          "topic_main_category": {
            "id": "Q8293925",
            "label": "Category:Bayesian inference"
          },
          "freebase_id": "/m/0d5kn",
          "P1296": "0281295",
          "P1051": "7719",
          "P138": {
            "id": "Q208452",
            "label": "Thomas Bayes"
          },
          "P2534": "P(H\\mid E) = \\frac{P(E\\mid H) \\cdot P(H)}{P(E)}",
          "quora_topic_id": "Bayesian-Inference",
          "P3827": [
            "bayesian-inference",
            "bayesian-analysis",
            "bayesian-confirmation-theory"
          ],
          "loc_id": "sh85012506",
          "mag_id": "160234255",
          "P2347": "17803",
          "P9100": "bayesian-inference",
          "P2179": "10003664",
          "P2004": "16108",
          "P8189": "987007282424705171",
          "openalex_id": "C160234255",
          "P5008": {
            "id": "Q6173448",
            "label": "Wikipedia:Vital articles/Level/4"
          },
          "P691": "ph135362",
          "P3222": "bayes-inferens",
          "openalex_topic_id": [
            "533808",
            "143282",
            "316492"
          ],
          "stack_exchange_tag": [
            "https://stats.stackexchange.com/tags/bayesian",
            "https://philosophy.stackexchange.com/tags/bayesian",
            "https://stackoverflow.com/tags/bayesian"
          ],
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "subclass_of": [
            "Q12718609",
            {
              "id": "Q938438",
              "label": "statistical inference"
            }
          ],
          "P12385": "inferencia-bayesiana",
          "commons_category": "Bayesian inference",
          "P1269": {
            "id": "Q812534",
            "label": "Bayesian probability"
          },
          "P13591": "concept/89628c4b-4062-4a41-9920-860f80a2eb33",
          "P1036": "519.542"
        }
      }
    },
    {
      "name": "Apprenticeship learning",
      "entity_type": "task",
      "relation": "proposes",
      "relevance": "The paper proposes efficient algorithms for the apprenticeship learning task, enabling learning of policies from expert demonstrations.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Apprenticeship_learning",
      "wikidata": {
        "id": "Q4781707",
        "label": "Apprenticeship learning",
        "description": "Concept in artificial intelligence",
        "aliases": [],
        "claims": {
          "mag_id": "2780254194",
          "freebase_id": "/m/04n1tc_"
        }
      }
    },
    {
      "name": "Preference elicitation",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "The paper identifies preference elicitation as a motivation for inverse reinforcement learning, where reward learning is valuable as an independent goal.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Preference_elicitation",
      "wikidata": {
        "id": "Q7239817",
        "label": "Preference elicitation",
        "description": "task of generating recommendations to a user",
        "aliases": [],
        "claims": {
          "mag_id": "2777868144",
          "freebase_id": "/m/07qkwh",
          "openalex_id": "C2777868144"
        }
      }
    },
    {
      "name": "Imitation learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Inverse reinforcement learning is closely related to imitation learning, both addressing learning from expert demonstrations.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Imitation_learning",
      "wikidata": {
        "id": "Q124742024",
        "label": "imitation learning",
        "description": "machine learning technique where agents learn from demonstrations",
        "aliases": [
          "behavior cloning",
          "behaviour cloning",
          "learning from demonstration"
        ],
        "claims": {
          "subclass_of": "Q7353390",
          "instance_of": "Q117348143",
          "P1269": "Q170978",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    }
  ]
}