{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Prompt engineering",
      "relation": "proposes",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Prompt_engineering",
      "relevance": "AutoPrompt proposes an automated method for creating prompts, advancing the field of prompt engineering.",
      "wikidata": {
        "id": "Q108941486",
        "label": "prompt engineering",
        "description": "creation or optimization of a prompt to be given to an artificial intelligence model",
        "aliases": [
          "prompt-based learning",
          "AI prompt engineering"
        ],
        "claims": {
          "subclass_of": [
            "Q110484020",
            {
              "id": "Q80006",
              "label": "computer programming"
            }
          ],
          "instance_of": [
            {
              "id": "Q2695280",
              "label": "technique"
            },
            {
              "id": "Q627436",
              "label": "field of work"
            }
          ],
          "P1269": {
            "id": "Q11660",
            "label": "artificial intelligence"
          },
          "use": [
            "Q65066631",
            "Q117246174"
          ],
          "P1056": {
            "id": "Q117217619",
            "label": "AI prompt"
          },
          "P5008": "Q117245199",
          "practiced_by": [
            "Q117674392",
            "Q123653520"
          ],
          "commons_category": "Prompt engineering for generative AI",
          "google_kg_id": "/g/11p6kpgt_n",
          "P691": "ph1266210"
        }
      }
    },
    {
      "name": "Gradient descent",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Gradient_descent",
      "relevance": "AutoPrompt uses gradient-guided search, which is based on gradient descent optimization.",
      "wikidata": {
        "id": "Q1199743",
        "label": "gradient descent",
        "description": "optimization algorithm",
        "aliases": [
          "steepest descent",
          "method of steepest descent",
          "hill-climbing algorithm"
        ],
        "claims": {
          "freebase_id": "/m/01cmhh",
          "commons_category": "Gradient descent",
          "quora_topic_id": "Gradient-Descent",
          "instance_of": [
            {
              "id": "Q2835765",
              "label": "optimization algorithm"
            },
            {
              "id": "Q2321565",
              "label": "iterative numerical method"
            }
          ],
          "P2534": "\\mathbf{x}_{n+1} = \\mathbf{x}_n -\\gamma\\nabla F(\\mathbf{x}_n)",
          "uses": "Q1868524",
          "mag_id": "153258448",
          "P2579": {
            "id": "Q141495",
            "label": "mathematical optimization"
          },
          "P9100": "gradient-descent",
          "openalex_id": "C153258448",
          "openalex_topic_id": "413756",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "subclass_of": {
            "id": "Q24034552",
            "label": "mathematical concept"
          },
          "use": "Q2539",
          "P2812": "MethodofSteepestDescent"
        }
      }
    },
    {
      "name": "BERT (language model)",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
      "relevance": "BERT is a key example of masked language models that AutoPrompt applies to extract knowledge and perform tasks.",
      "wikidata": {
        "id": "Q61726893",
        "label": "bidirectional encoder representations from transformers",
        "description": "deep learning artificial neural network language model",
        "aliases": [
          "BERT",
          "bidirectional encoder representations from transformer"
        ],
        "claims": {
          "described_by_source": "Q57267388",
          "P571": "2018-00-00",
          "P3575": [
            "+110000000",
            "+340000000"
          ],
          "instance_of": [
            {
              "id": "Q115305900",
              "label": "large language model"
            },
            "Q85810444",
            "Q24868018"
          ],
          "developer": {
            "id": "Q28943742",
            "label": "Google Research"
          },
          "P1324": "https://github.com/google-research/bert",
          "P275": {
            "id": "Q13785927",
            "label": "Apache Software License 2.0"
          },
          "use": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "google_kg_id": "/g/11h75tkhxs",
          "P9100": "bert",
          "P138": {
            "id": "Q584184",
            "label": "Bert"
          },
          "P6216": {
            "id": "Q50423863",
            "label": "copyrighted"
          },
          "P973": [
            "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html",
            "https://devopedia.org/bert-language-model"
          ],
          "subclass_of": "Q85810444",
          "P856": "https://arxiv.org/abs/1810.04805",
          "commons_category": "BERT"
        }
      }
    },
    {
      "name": "Cloze test",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Cloze_test",
      "relevance": "AutoPrompt reformulates tasks as fill-in-the-blanks problems (cloze tests) to gauge language model knowledge.",
      "wikidata": {
        "id": "Q951968",
        "label": "cloze test",
        "description": "\"Fill-in-the-blank\" language learning technique",
        "aliases": [
          "cloze deletion test",
          "cloze deletion",
          "Cloze procedure"
        ],
        "claims": {
          "mag_id": "2779286901",
          "openalex_id": "C2779286901",
          "freebase_id": "/m/027d7fs",
          "image": "Student matching cloze terms on smartboard.jpg",
          "subclass_of": {
            "id": "Q27318",
            "label": "test"
          },
          "P9497": "4615",
          "commons_category": "Cloze test"
        }
      }
    },
    {
      "name": "Sentiment analysis",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Sentiment_analysis",
      "relevance": "AutoPrompt demonstrates MLMs can perform sentiment analysis without fine-tuning using automatically generated prompts.",
      "wikidata": {
        "id": "Q2271421",
        "label": "sentiment analysis",
        "description": "use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials",
        "aliases": [
          "opinion mining",
          "Opinion Extraction"
        ],
        "claims": {
          "freebase_id": "/m/0g57xn",
          "quora_topic_id": "Sentiment-Analysis",
          "P3827": "sentiment-analysis",
          "subclass_of": [
            "Q1185804",
            {
              "id": "Q30642",
              "label": "natural language processing"
            }
          ],
          "commons_category": "Sentiment analysis",
          "P2179": "10003353",
          "P2579": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "use": "Q39809",
          "instance_of": {
            "id": "Q627436",
            "label": "field of work"
          },
          "mag_id": "66402592",
          "stack_exchange_tag": [
            "https://datascience.stackexchange.com/tags/sentiment-analysis",
            "https://stackoverflow.com/tags/sentiment-analysis"
          ],
          "uses": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "P3553": "19966010",
          "P9100": "sentiment-analysis",
          "openalex_id": "C66402592",
          "mesh_id": "D000090042",
          "mesh_tree_code": [
            "G17.035.250.750",
            "L01.224.050.375.815",
            "L01.313.500.750.280.199.750",
            "L01.470.625.750"
          ],
          "openalex_topic_id": "219676",
          "P7502": "Sentiment_analysis-V48YAR",
          "P2892": "C5544431",
          "P9309": "sentimentAnalysis",
          "P8168": "Q1194520",
          "P13397": "sentiment+analysis"
        }
      }
    },
    {
      "name": "Textual entailment",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Textual_entailment",
      "relevance": "AutoPrompt shows MLMs can perform natural language inference (textual entailment) without additional parameters.",
      "wikidata": {
        "id": "Q6588467",
        "label": "textual entailment",
        "description": "concept in natural language processing",
        "aliases": [
          "natural language inference"
        ],
        "claims": {
          "P2579": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "mag_id": "95318506",
          "freebase_id": "/m/0h3mz8c",
          "openalex_id": "C95318506",
          "P3712": {
            "id": "Q1394144",
            "label": "automatic summarization"
          }
        }
      }
    },
    {
      "name": "Relationship extraction",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Relationship_extraction",
      "relevance": "AutoPrompt demonstrates MLMs can be used as relation extractors more effectively than supervised models.",
      "wikidata": {
        "id": "Q7310755",
        "label": "relationship extraction",
        "description": "type of text mining",
        "aliases": [
          "relation extraction",
          "Relation Extraction",
          "relational extraction"
        ],
        "claims": {
          "mag_id": "153604712",
          "subclass_of": "Q676880",
          "freebase_id": "/m/02rhpj1",
          "openalex_id": "C153604712",
          "openalex_topic_id": "443370",
          "instance_of": [
            "Q11862829",
            {
              "id": "Q1047113",
              "label": "field of study"
            },
            "Q2267705"
          ]
        }
      }
    },
    {
      "name": "Benchmark (computing)",
      "relation": "uses",
      "entity_type": "data",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Benchmark_(computing)",
      "relevance": "AutoPrompt evaluates on the LAMA benchmark to assess factual knowledge extraction from language models.",
      "wikidata": {
        "id": "Q816747",
        "label": "benchmark",
        "description": "test to measure the performance of a computer system or component",
        "aliases": [
          "performance benchmark",
          "hardware benchmark",
          "performance test",
          "computer benchmark",
          "software benchmark",
          "benchmarking"
        ],
        "claims": {
          "freebase_id": "/m/06bpl_",
          "topic_main_category": "Q7013673",
          "commons_category": "Benchmarks (computing)",
          "subclass_of": "Q1047213",
          "mag_id": "137955351",
          "P8408": "Benchmark",
          "P2575": [
            {
              "id": "Q1197550",
              "label": "computer performance"
            },
            "Q1576430"
          ],
          "P1557": {
            "id": "Q104537917",
            "label": "computer benchmarking"
          },
          "P3553": "19661744",
          "openalex_topic_id": "212912",
          "P6706": "benchmark",
          "P3365": "bench-mark",
          "P5844": "benchmark",
          "P691": "ph237420",
          "P13572": "benchmark",
          "P12873": [
            "Benchmarking_(Español)",
            "Benchmarking_(Русский)"
          ]
        }
      }
    },
    {
      "name": "Language model",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model",
      "relevance": "Language models are the core foundation that AutoPrompt studies and applies prompts to.",
      "wikidata": {
        "id": "Q3621696",
        "label": "language model",
        "description": "probabilistic model of a natural or formal language, or generally of elements of signal sequences",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/065lv0",
          "mag_id": "137293760",
          "subclass_of": "Q3284399",
          "P1269": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "P8408": "LanguageModeling",
          "openalex_id": "C137293760",
          "P10407": "923-2",
          "use": "Q96407327",
          "openalex_topic_id": "184201",
          "topic_main_category": "Q16500316",
          "P12086": "Taalmodel",
          "P8309": "18-332728",
          "P1368": "000355288",
          "P13397": "language+model",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    }
  ]
}