{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Reinforcement learning",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "relevance": "The core machine learning paradigm that Decision Transformer reimagines as a sequence modeling problem.",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Transformer (deep learning)",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
      "relevance": "The foundational neural network architecture that Decision Transformer leverages for its RL application.",
      "wikidata": {
        "id": "Q85810444",
        "label": "transformer",
        "description": "machine-learning model architecture first developed by Google Brain",
        "aliases": [
          "transformer model",
          "transformer architecture",
          "transformers"
        ],
        "claims": {
          "subclass_of": [
            "Q192776",
            {
              "id": "Q113364611",
              "label": "deep learning model"
            }
          ],
          "developer": [
            {
              "id": "Q16927616",
              "label": "Google Brain"
            },
            {
              "id": "Q44749723",
              "label": "Ashish Vaswani"
            },
            "Q30251943"
          ],
          "follows": [
            {
              "id": "Q1457734",
              "label": "recurrent neural network"
            },
            {
              "id": "Q6673524",
              "label": "long short-term memory"
            }
          ],
          "described_by_source": {
            "id": "Q30249683",
            "label": "Attention Is All You Need"
          },
          "google_kg_id": "/g/11hz_m4ssw",
          "has_parts": [
            "Q42586063",
            {
              "id": "Q745243",
              "label": "decoder"
            }
          ],
          "uses": [
            {
              "id": "Q103701642",
              "label": "attention"
            },
            {
              "id": "Q5441227",
              "label": "feedforward neural network"
            }
          ],
          "use": [
            {
              "id": "Q30642",
              "label": "natural language processing"
            },
            {
              "id": "Q844240",
              "label": "computer vision"
            },
            {
              "id": "Q3245113",
              "label": "statistical machine translation"
            },
            {
              "id": "Q1394144",
              "label": "automatic summarization"
            }
          ],
          "schematic": "Transformer, full architecture.png",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/transformer",
          "P575": "2017-06-12",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Generative pre-trained transformer",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
      "relevance": "GPT models demonstrate the scalability of transformer-based language modeling that inspires Decision Transformer's approach.",
      "wikidata": {
        "id": "Q116777014",
        "label": "generative pre-trained transformer",
        "description": "type of large language model",
        "aliases": [
          "GPT",
          "generative pretrained transformer"
        ],
        "claims": {
          "has_parts": [
            {
              "id": "Q116937684",
              "label": "GPT-J"
            },
            "Q115564437",
            {
              "id": "Q112702082",
              "label": "foundation model"
            },
            "Q116709136",
            {
              "id": "Q95726718",
              "label": "GPT-1"
            },
            "Q95726734",
            "Q95726727",
            {
              "id": "Q117791942",
              "label": "AutoGPT"
            },
            "Q116793893",
            {
              "id": "Q105078662",
              "label": "DALL-E"
            },
            "Q108582200",
            {
              "id": "Q118176939",
              "label": "Open Assistant"
            }
          ],
          "P5008": "Q117245199",
          "subclass_of": [
            {
              "id": "Q115305900",
              "label": "large language model"
            },
            "Q117246174",
            "Q85810444",
            "Q2539",
            {
              "id": "Q11660",
              "label": "artificial intelligence"
            }
          ],
          "different_from": "Q124237017",
          "short_name": "GPT",
          "google_kg_id": "/g/11ts8q7891",
          "P8313": "GPT",
          "P5019": "generative-pre-trained-transformer-informatik",
          "P11662": "3641756"
        }
      }
    },
    {
      "name": "BERT (language model)",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/BERT_(language_model)",
      "relevance": "BERT exemplifies advances in language modeling with transformer-based pre-training that Decision Transformer builds upon.",
      "wikidata": {
        "id": "Q61726893",
        "label": "bidirectional encoder representations from transformers",
        "description": "deep learning artificial neural network language model",
        "aliases": [
          "BERT",
          "bidirectional encoder representations from transformer"
        ],
        "claims": {
          "described_by_source": "Q57267388",
          "P571": "2018-00-00",
          "P3575": [
            "+110000000",
            "+340000000"
          ],
          "instance_of": [
            {
              "id": "Q115305900",
              "label": "large language model"
            },
            "Q85810444",
            "Q24868018"
          ],
          "developer": {
            "id": "Q28943742",
            "label": "Google Research"
          },
          "P1324": "https://github.com/google-research/bert",
          "P275": {
            "id": "Q13785927",
            "label": "Apache Software License 2.0"
          },
          "use": {
            "id": "Q30642",
            "label": "natural language processing"
          },
          "google_kg_id": "/g/11h75tkhxs",
          "P9100": "bert",
          "P138": {
            "id": "Q584184",
            "label": "Bert"
          },
          "P6216": {
            "id": "Q50423863",
            "label": "copyrighted"
          },
          "P973": [
            "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html",
            "https://devopedia.org/bert-language-model"
          ],
          "subclass_of": "Q85810444",
          "P856": "https://arxiv.org/abs/1810.04805",
          "commons_category": "BERT"
        }
      }
    },
    {
      "name": "Policy gradient method",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Policy_gradient_method",
      "relevance": "Traditional policy gradient methods represent the prior RL approach that Decision Transformer moves away from.",
      "wikidata": {
        "id": "Q113840014",
        "label": "policy-gradient method",
        "description": "class of reinforcement learning algorithms",
        "aliases": [
          "policy gradient method",
          "policy gradient"
        ],
        "claims": {
          "subclass_of": {
            "id": "Q830687",
            "label": "reinforcement learning"
          },
          "P9526": "Policy_gradient_methods",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Autoregressive model",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Autoregressive_model",
      "relevance": "The autoregressive modeling principle is the core of Decision Transformer's causally masked sequence generation.",
      "wikidata": {
        "id": "Q2202883",
        "label": "autoregressive model",
        "description": "representation of a type of random process",
        "aliases": [
          "AR model",
          "autoregressive filter"
        ],
        "claims": {
          "freebase_id": "/m/04y5r84",
          "stack_exchange_tag": "https://stackoverflow.com/tags/autoregressive-models",
          "mag_id": "159877910",
          "subclass_of": [
            "Q3284399",
            "Q192776",
            "Q208042",
            {
              "id": "Q3072260",
              "label": "filter"
            }
          ],
          "openalex_id": "C159877910",
          "instance_of": "Q120281892",
          "P2924": "1850334"
        }
      }
    },
    {
      "name": "Atari",
      "relation": "uses",
      "entity_type": "data",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Atari_Games",
      "relevance": "Atari games are a standard benchmark for evaluating RL algorithms that Decision Transformer is tested on.",
      "wikidata": {
        "id": "Q1061580",
        "label": "Atari Games",
        "description": "American former producer of arcade games, and originally part of Atari, Inc.",
        "aliases": [
          "Atari Games Corporation",
          "Atari Games, Corp.",
          "AT Games Inc.",
          "Midway Games West, Inc.",
          "Midway Games West"
        ],
        "claims": {
          "instance_of": [
            {
              "id": "Q210167",
              "label": "video game developer"
            },
            {
              "id": "Q1137109",
              "label": "video game publisher"
            }
          ],
          "follows": {
            "id": "Q13409231",
            "label": "Atari, Inc."
          },
          "P856": "https://www.atari.com/",
          "freebase_id": "/m/01scmq",
          "P452": {
            "id": "Q941594",
            "label": "video game industry"
          },
          "P17": "Q30",
          "P159": "Q927510",
          "P571": "1984-07-01",
          "P154": "Atari Games Logo.svg",
          "P576": [
            "2003-02-01",
            "2013-10-02"
          ],
          "P127": [
            {
              "id": "Q247032",
              "label": "Warner Communications"
            },
            {
              "id": "Q309996",
              "label": "Namco"
            },
            {
              "id": "Q191715",
              "label": "WarnerMedia"
            },
            "Q1403157",
            {
              "id": "Q2319420",
              "label": "Warner Bros. Games"
            }
          ],
          "topic_main_category": "Q6461577",
          "commons_category": "Atari video games",
          "P800": [
            "Q2007458",
            {
              "id": "Q1053648",
              "label": "Centipede"
            },
            {
              "id": "Q1192794",
              "label": "Missile Command"
            },
            {
              "id": "Q177491",
              "label": "Breakout"
            },
            "Q2617620",
            {
              "id": "Q216293",
              "label": "Pong"
            },
            {
              "id": "Q88240",
              "label": "720°"
            }
          ],
          "P1320": [
            "us_md/F04190963",
            "us_de/5475677"
          ],
          "P355": [
            {
              "id": "Q2536056",
              "label": "Tengen"
            },
            "Q64447391"
          ],
          "P749": [
            {
              "id": "Q247032",
              "label": "Warner Communications"
            },
            {
              "id": "Q309996",
              "label": "Namco"
            },
            "Q7805202",
            {
              "id": "Q149947",
              "label": "Midway Games"
            }
          ],
          "P4773": [
            "atari-inc_",
            "midway-games-west-inc"
          ],
          "P7634": "1",
          "P1056": {
            "id": "Q3966",
            "label": "computer hardware"
          },
          "founded_by": {
            "id": "Q191715",
            "label": "WarnerMedia"
          },
          "P7886": "C47349",
          "P268": "14042329k",
          "P214": "125709381",
          "P5531": "0001725056",
          "P1297": "82-2731167",
          "P6262": [
            "gauntlet:Atari_Games",
            "gamicus:Atari_Games",
            "video-games:Atari_Games"
          ],
          "P5247": "3010-794",
          "P4264": "atari-games",
          "P7784": "124",
          "P7642": "70",
          "P7570": "2396",
          "P7521": "44",
          "P6182": "12625",
          "babelnet_id": "02398667n",
          "P7502": "Atari_Games",
          "P2088": "atari",
          "P11689": "32826",
          "P13695": "9rx"
        }
      }
    },
    {
      "name": "Attention (machine learning)",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
      "relevance": "Attention mechanisms are the key architectural component that enables the Transformer model used in Decision Transformer.",
      "wikidata": {
        "id": "Q103701642",
        "label": "attention",
        "description": "machine learning technique",
        "aliases": [
          "attention mechanism"
        ],
        "claims": {
          "subclass_of": "Q6501338",
          "P1269": "Q2539",
          "google_kg_id": "/g/11qpclnpwr",
          "use": [
            "Q192776",
            {
              "id": "Q25325414",
              "label": "neural Turing machine"
            },
            {
              "id": "Q28324912",
              "label": "differentiable neural computer"
            },
            "Q85810444",
            "Q108281205"
          ],
          "P1557": "Q6501338",
          "openalex_topic_id": "413584",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/attention",
          "schematic": "Attention mechanism overview.svg"
        }
      }
    },
    {
      "name": "Seq2seq",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Seq2seq",
      "relevance": "Sequence-to-sequence modeling principles are foundational to Decision Transformer's casting of RL as sequence modeling.",
      "wikidata": {
        "id": "Q41589189",
        "label": "sequence-to-sequence learning",
        "description": "family of machine learning approaches",
        "aliases": [
          "seq2seq",
          "Seq2seq"
        ],
        "claims": {
          "subclass_of": "Q2539",
          "google_kg_id": "/g/11j4djqh5n",
          "use": [
            {
              "id": "Q1394144",
              "label": "automatic summarization"
            },
            "Q79798",
            "Q870780"
          ]
        }
      }
    },
    {
      "name": "Offline learning",
      "relation": "proposes",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Offline_learning",
      "relevance": "Decision Transformer proposes a novel transformer-based approach to offline reinforcement learning that outperforms prior methods.",
      "wikidata": {
        "id": "Q7079636",
        "label": "offline machine learning",
        "description": "method of machine learning",
        "aliases": [
          "batch learning",
          "cumulated modification learning",
          "epochal learning",
          "block learning"
        ],
        "claims": {
          "subclass_of": "Q2539",
          "opposite_of": "Q7094097",
          "mag_id": "2780490138",
          "freebase_id": "/m/02qnykr",
          "openalex_id": "C2780490138"
        }
      }
    }
  ]
}