{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Partially observable Markov decision process",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper addresses gradient-based optimization directly on the policy performance in POMDPs, which is the core problem domain.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process",
      "wikidata": {
        "id": "Q176814",
        "label": "partially observable Markov decision process",
        "description": "generalization of a Markov decision process",
        "aliases": [
          "POMDP"
        ],
        "claims": {
          "freebase_id": "/m/08p0k3",
          "P138": "Q176659",
          "P144": "Q176789",
          "mag_id": "17098449",
          "openalex_id": "C17098449",
          "openalex_topic_id": "41685"
        }
      }
    },
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper applies reinforcement learning methodology to develop gradient-based policy optimization techniques.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": "Q2539",
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": "Q2539",
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            {
              "id": "Q111862379",
              "label": "machine learning method"
            },
            {
              "id": "Q130609847",
              "label": "learning approach"
            }
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Gradient descent",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Gradient ascent (the dual of gradient descent) is the core optimization technique underlying the GPOMDP algorithm.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Gradient_descent",
      "wikidata": {
        "id": "Q1199743",
        "label": "gradient descent",
        "description": "optimization algorithm",
        "aliases": [
          "steepest descent",
          "method of steepest descent",
          "hill-climbing algorithm"
        ],
        "claims": {
          "freebase_id": "/m/01cmhh",
          "commons_category": "Gradient descent",
          "quora_topic_id": "Gradient-Descent",
          "instance_of": [
            {
              "id": "Q2835765",
              "label": "optimization algorithm"
            },
            {
              "id": "Q2321565",
              "label": "iterative numerical method"
            }
          ],
          "P2534": "\\mathbf{x}_{n+1} = \\mathbf{x}_n -\\gamma\\nabla F(\\mathbf{x}_n)",
          "uses": "Q1868524",
          "mag_id": "153258448",
          "P2579": {
            "id": "Q141495",
            "label": "mathematical optimization"
          },
          "P9100": "gradient-descent",
          "openalex_id": "C153258448",
          "openalex_topic_id": "413756",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "subclass_of": {
            "id": "Q24034552",
            "label": "mathematical concept"
          },
          "use": "Q2539",
          "P2812": "MethodofSteepestDescent"
        }
      }
    },
    {
      "name": "Policy gradient method",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "GPOMDP is a novel REINFORCE-like algorithm that estimates policy gradients for average reward optimization in POMDPs.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Policy_gradient_method",
      "wikidata": {
        "id": "Q113840014",
        "label": "policy-gradient method",
        "description": "class of reinforcement learning algorithms",
        "aliases": [
          "policy gradient method",
          "policy gradient"
        ],
        "claims": {
          "subclass_of": {
            "id": "Q830687",
            "label": "reinforcement learning"
          },
          "P9526": "Policy_gradient_methods",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Markov chain",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper explicitly uses a single sample path of the underlying Markov chain for gradient estimation.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_chain",
      "wikidata": {
        "id": "Q176645",
        "label": "Markov chain",
        "description": "stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event",
        "aliases": [
          "Markov process",
          "Markov chains"
        ],
        "claims": {
          "P138": "Q176659",
          "P508": "19104",
          "gnd_id": "4134948-9",
          "freebase_id": "/m/0gjhm",
          "mesh_id": "D008390",
          "subclass_of": [
            "Q486902",
            "Q2221775"
          ],
          "P3285": "60J10",
          "quora_topic_id": "Markov-Chains",
          "P3827": [
            "transition-probabilities",
            "markov-chains"
          ],
          "P2812": "MarkovChain",
          "britannica_id": "topic/Markov-chain",
          "commons_category": "Markov chains",
          "image": "Markovkate 01.svg",
          "instance_of": "Q3284399",
          "mesh_tree_code": [
            "E05.318.740.600.500",
            "E05.318.740.996.500",
            "G17.830.500",
            "N05.715.360.750.625.500",
            "N05.715.360.750.770.500",
            "N06.850.520.830.600.500",
            "N06.850.520.830.996.500"
          ],
          "P1245": "382987",
          "mag_id": "98763669",
          "P4215": "Markov chain",
          "P4613": [
            "62430",
            "63754"
          ],
          "P5106": "markovchain",
          "P2924": "2187018",
          "loc_id": "sh85081369",
          "P2347": "13075",
          "P8189": "987007553386405171",
          "openalex_id": [
            "C159886148",
            "C98763669"
          ],
          "openalex_topic_id": [
            "235388",
            "296332",
            "500029"
          ],
          "P5008": {
            "id": "Q6173448",
            "label": "Wikipedia:Vital articles/Level/4"
          },
          "P2163": "1010347",
          "P268": "11932425d",
          "P10376": [
            "agricultural-and-biological-sciences/markov-chain",
            "computer-science/markov-chain",
            "earth-and-planetary-sciences/markov-chain",
            "economics-econometrics-and-finance/markov-chain",
            "materials-science/markov-chain",
            "mathematics/markov-chain",
            "physics-and-astronomy/markov-chain",
            "psychology/markov-chain",
            "social-sciences/markov-chain"
          ],
          "P2892": "C0024828",
          "P950": "XX540042",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "P6564": "markov-chains",
          "P3911": "15220-4",
          "P13591": "concept/675ba6ed-5b3f-46ec-8802-b3440b40a160",
          "P1036": "519.233"
        }
      }
    },
    {
      "name": "Conjugate gradient method",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "The paper shows how gradient estimates from GPOMDP can be integrated into conjugate gradient procedures to find local optima.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Conjugate_gradient_method",
      "wikidata": {
        "id": "Q1191895",
        "label": "conjugate gradient method",
        "description": "method to compute systems of linear equations whose matrix is symmetric positive-definite",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/052fr3",
          "instance_of": {
            "id": "Q4380101",
            "label": "projection method for solving system of linear equations"
          },
          "P3827": "conjugate-gradient-method",
          "described_by_source": [
            "Q27721407",
            {
              "id": "Q15428802",
              "label": "Journal of Inequalities and Applications"
            }
          ],
          "P61": [
            {
              "id": "Q983286",
              "label": "Magnus Hestenes"
            },
            {
              "id": "Q122724",
              "label": "Eduard Stiefel"
            }
          ],
          "gnd_id": "4255670-3",
          "loc_id": "sh85031141",
          "P268": "12168447j",
          "P2163": "875318",
          "P269": "030223253",
          "mag_id": "81184566",
          "openalex_id": "C81184566",
          "P8189": "987007555420405171",
          "P2812": "ConjugateGradientMethod",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "P13591": "concept/8d0f225c-faeb-45b6-9511-af9e4f1e748a"
        }
      }
    }
  ]
}