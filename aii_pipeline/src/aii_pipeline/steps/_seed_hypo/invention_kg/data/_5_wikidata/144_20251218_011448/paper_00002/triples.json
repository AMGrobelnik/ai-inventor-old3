{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Core machine learning paradigm that MOPO builds upon for offline policy learning.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "wikidata": {
        "id": "Q830687",
        "label": "reinforcement learning",
        "description": "type of machine learning where an agent learns how to behave in an environment by performing actions and receiving rewards or penalties in return, aiming to maximize the cumulative reward over time",
        "aliases": [
          "RL"
        ],
        "claims": {
          "freebase_id": "/m/0hjlw",
          "stack_exchange_tag": [
            "https://stackoverflow.com/tags/reinforcement-learning",
            "https://ai.stackexchange.com/tags/reinforcement-learning"
          ],
          "part_of": {
            "id": "Q2539",
            "label": "machine learning"
          },
          "quora_topic_id": "Reinforcement-Learning",
          "subclass_of": {
            "id": "Q2539",
            "label": "machine learning"
          },
          "babelnet_id": "03511335n",
          "P2179": "10010261",
          "mag_id": "97541855",
          "P7502": [
            "Reinforcement_Learning-R39",
            "Reinforcement_Learning"
          ],
          "P9100": "reinforcement-learning",
          "P9526": "Reinforcement_learning",
          "P508": "69813",
          "P268": "17127232k",
          "loc_id": "sh92000704",
          "gnd_id": "4825546-4",
          "P8189": "987007546785305171",
          "P8529": "461105",
          "openalex_id": "C97541855",
          "P10": "A-novel-approach-to-locomotion-learning-Actor-Critic-architecture-using-central-pattern-generators-Movie1.ogv",
          "openalex_topic_id": [
            "216762",
            "121743",
            "504466"
          ],
          "commons_category": "Reinforcement learning",
          "P6009": "32055",
          "P3984": "reinforcementlearning",
          "instance_of": [
            "Q111862379",
            "Q130609847"
          ],
          "P8687": "+30816",
          "P10376": [
            "computer-science/reinforcement-learning",
            "neuroscience/reinforcement-learning"
          ],
          "topic_main_category": "Q87071489",
          "P8885": "강화학습",
          "P1149": "Q325.6",
          "mesh_id": "D000098408",
          "mesh_tree_code": [
            "G17.035.250.500.485",
            "L01.224.050.375.530.485"
          ],
          "described_by_source": "Q133280541",
          "different_from": {
            "id": "Q123916004",
            "label": "inverse reinforcement learning"
          },
          "P3235": "reinforcement-learning",
          "P13591": "concept/62f2752a-56e6-44ef-ac2f-b8bb40173d38",
          "P1036": "006.31"
        }
      }
    },
    {
      "name": "Policy gradient method",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Fundamental optimization approach for policy-based reinforcement learning algorithms.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Policy_gradient_method",
      "wikidata": {
        "id": "Q113840014",
        "label": "policy-gradient method",
        "description": "class of reinforcement learning algorithms",
        "aliases": [
          "policy gradient method",
          "policy gradient"
        ],
        "claims": {
          "subclass_of": {
            "id": "Q830687",
            "label": "reinforcement learning"
          },
          "P9526": "Policy_gradient_methods",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Markov decision process",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Theoretical framework underlying the MDP formulation that MOPO optimizes a lower bound for.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "wikidata": {
        "id": "Q176789",
        "label": "Markov decision process",
        "description": "mathematical model for sequential decision making under uncertainty",
        "aliases": [
          "MDP",
          "MDPs"
        ],
        "claims": {
          "P138": "Q176659",
          "freebase_id": "/m/048gl8",
          "P1051": "7713",
          "P4969": {
            "id": "Q176814",
            "label": "partially observable Markov decision process"
          },
          "mag_id": "106189395",
          "openalex_id": "C106189395",
          "subclass_of": [
            "Q1331926",
            "Q176737"
          ],
          "openalex_topic_id": "56113",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Model-free (reinforcement learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Baseline approach that MOPO compares against; MOPO demonstrates advantages over model-free methods.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)",
      "wikidata": {
        "id": "Q63788448",
        "label": "model-free reinforcement learning",
        "description": "type of machine learning algorithm",
        "aliases": [],
        "claims": {
          "google_kg_id": "/g/11fjzc3qzm",
          "subclass_of": {
            "id": "Q830687",
            "label": "reinforcement learning"
          },
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Domain adaptation",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Related concept addressing distributional shift, the core challenge MOPO tackles through uncertainty penalization.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Domain_adaptation",
      "wikidata": {
        "id": "Q19246213",
        "label": "Domain Adaptation",
        "description": "field associated with machine learning and transfer learning",
        "aliases": [],
        "claims": {
          "mag_id": "2776434776",
          "freebase_id": "/m/012vqfs3",
          "P9100": "domain-adaptation",
          "openalex_id": "C2776434776",
          "subclass_of": {
            "id": "Q830687",
            "label": "reinforcement learning"
          }
        }
      }
    },
    {
      "name": "Control theory",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Foundational theory for continuous control tasks that MOPO evaluates on as part of its benchmarks.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Control_theory",
      "wikidata": {
        "id": "Q6501221",
        "label": "control theory",
        "description": "branch of engineering and mathematics that deals with the behavior of dynamical systems with inputs, and how their behavior is modified by feedback",
        "aliases": [
          "control systems",
          "controls"
        ],
        "claims": {
          "subclass_of": "Q11984976",
          "commons_category": "Control theory",
          "P508": "4960",
          "topic_main_category": {
            "id": "Q5881968",
            "label": "Category:Control theory"
          },
          "P349": "00570298",
          "freebase_id": "/m/020h8",
          "stack_exchange_tag": "https://stackoverflow.com/tags/control-theory",
          "britannica_id": "topic/control-theory-mathematics",
          "P2812": "ControlTheory",
          "gnd_id": "4032317-1",
          "study_of": [
            "Q638328",
            "Q183635"
          ],
          "P935": "Control theory",
          "instance_of": [
            {
              "id": "Q1936384",
              "label": "area of mathematics"
            },
            {
              "id": "Q682496",
              "label": "systems engineering"
            }
          ],
          "P3827": "control-theory",
          "quora_topic_id": "Control-Theory",
          "P3569": "techologie-en-techniek/regeltechniek",
          "topic_maintained_by": "Q7648512",
          "P1296": "0171756",
          "followed_by": "Q123637",
          "mag_id": "65244806",
          "P3553": "19803111",
          "openalex_id": "C2984730371",
          "loc_id": "sh85031658",
          "P2347": "868",
          "P691": "ph116433",
          "P8189": "987007557820705171",
          "part_of": "Q113208892",
          "P5008": {
            "id": "Q6173448",
            "label": "Wikipedia:Vital articles/Level/4"
          },
          "P3911": "15555-1",
          "P6104": {
            "id": "Q8487137",
            "label": "WikiProject Mathematics"
          },
          "P12385": "teoria-de-control",
          "P7666": "vadybos-teorijos",
          "P8885": "제어",
          "P4644": "3c42d13b-d6e4-4ed9-bdc4-991251461dc4",
          "P13591": "concept/859c9ff6-1db4-4cc0-a908-8b9345e9c253",
          "P10380": "control-theory",
          "P1036": [
            "003.5",
            "515.642"
          ]
        }
      }
    },
    {
      "name": "Uncertainty quantification",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Key technique that MOPO employs to penalize model uncertainty and avoid distributional shift.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Uncertainty_quantification",
      "wikidata": {
        "id": "Q7882499",
        "label": "uncertainty quantification",
        "description": "characterization and reduction of uncertainties in both computational and real world applications",
        "aliases": [
          "UQ",
          "Uncertainty quantification model"
        ],
        "claims": {
          "subclass_of": {
            "id": "Q29205",
            "label": "uncertainty analysis"
          },
          "babelnet_id": "03426783n",
          "mag_id": "32230216",
          "P1269": {
            "id": "Q13649246",
            "label": "uncertainty"
          },
          "freebase_id": "/m/0fj8wc",
          "openalex_id": "C32230216"
        }
      }
    },
    {
      "name": "Model predictive control",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Control approach that MOPO adapts for offline policy optimization using learned dynamics models.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Model_predictive_control",
      "wikidata": {
        "id": "Q1782962",
        "label": "model predictive control",
        "description": "advanced method of process control",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/04604p",
          "mag_id": "172205157",
          "openalex_id": "C172205157",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Offline learning",
      "entity_type": "concept",
      "relation": "proposes",
      "relevance": "The paper proposes MOPO, a novel model-based offline policy optimization method that improves offline learning in RL with uncertainty penalization.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Offline_learning",
      "wikidata": {
        "id": "Q7079636",
        "label": "offline machine learning",
        "description": "method of machine learning",
        "aliases": [
          "batch learning",
          "cumulated modification learning",
          "epochal learning",
          "block learning"
        ],
        "claims": {
          "subclass_of": {
            "id": "Q2539",
            "label": "machine learning"
          },
          "opposite_of": {
            "id": "Q7094097",
            "label": "online machine learning"
          },
          "mag_id": "2780490138",
          "freebase_id": "/m/02qnykr",
          "openalex_id": "C2780490138"
        }
      }
    }
  ]
}