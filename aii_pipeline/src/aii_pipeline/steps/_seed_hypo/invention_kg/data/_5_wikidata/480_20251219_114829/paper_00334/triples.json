{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Mixture of experts",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Core architecture that Uni-MoE extends to multimodal settings for efficient conditional computation.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Mixture_of_experts",
      "wikidata": {
        "id": "Q30688561",
        "label": "mixture of experts",
        "description": "machine learning technique",
        "aliases": [
          "MoE"
        ],
        "claims": {
          "mag_id": "2778025020",
          "google_kg_id": "/g/11dzt5tj98",
          "P1269": [
            "Q2539",
            {
              "id": "Q245652",
              "label": "ensemble learning"
            }
          ],
          "instance_of": [
            "Q3284399",
            {
              "id": "Q2260434",
              "label": "mixture model"
            }
          ],
          "P61": [
            {
              "id": "Q59678968",
              "label": "Robert A. Jacobs"
            },
            {
              "id": "Q3308285",
              "label": "Michael I. Jordan"
            },
            "Q102215113",
            {
              "id": "Q92894",
              "label": "Geoffrey Hinton"
            }
          ]
        }
      }
    },
    {
      "name": "Large language model",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Foundation model type that Uni-MoE enhances with multimodal capabilities and MoE efficiency.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "wikidata": {
        "id": "Q115305900",
        "label": "large language model",
        "description": "language model built with very large amounts of texts",
        "aliases": [
          "LLM",
          "LLMs",
          "large language models",
          "word soup machine",
          "word soup model",
          "word salad machine",
          "word salad model"
        ],
        "claims": {
          "subclass_of": "Q3621696",
          "P1269": {
            "id": "Q11660",
            "label": "artificial intelligence"
          },
          "short_name": "LLM",
          "uses": [
            {
              "id": "Q117217619",
              "label": "AI prompt"
            },
            "Q85810444"
          ],
          "P5008": "Q117245199",
          "topic_main_category": {
            "id": "Q117280639",
            "label": "Category:Large language models"
          },
          "has_parts": [
            {
              "id": "Q116777014",
              "label": "generative pre-trained transformer"
            },
            "Q117246174",
            "Q3614994"
          ],
          "P973": "https://www.youtube.com/watch?v=WqYBx2gB6vA",
          "P5844": "neo-modello-linguistico-di-grandi-dimensioni_(Neologismi)",
          "opposite_of": "Q123759530",
          "google_kg_id": "/g/11kc9956b3",
          "gnd_id": "1322631905",
          "P12861": "E434",
          "use": [
            "Q116961403",
            {
              "id": "Q3510521",
              "label": "computer security"
            }
          ],
          "instance_of": {
            "id": "Q117349475",
            "label": "artificial intelligence model type"
          },
          "commons_category": "Large language models",
          "britannica_id": "topic/large-language-model",
          "P989": [
            "Wikipedia - Large language model (spoken by AI voice).mp3",
            "Wikipedia - Modelo extenso de lenguaje (hablado por voz AI).mp3"
          ],
          "mesh_id": "D000098342",
          "mesh_tree_code": [
            "G17.035.250.500.250.500",
            "G17.485.500.500",
            "L01.224.050.375.530.250.500",
            "L01.224.050.375.605.500.500",
            "L01.224.494"
          ],
          "P3911": "30478-6",
          "P2354": {
            "id": "Q131598147",
            "label": "list of large language models"
          },
          "P1368": "000355290",
          "described_by_source": "Q133280509",
          "P508": "77644",
          "P13572": "large+language+model",
          "stack_exchange_tag": [
            "https://genai.stackexchange.com/tags/llm",
            "https://ai.stackexchange.com/tags/large-language-models",
            "https://stats.stackexchange.com/tags/llm",
            "https://stackoverflow.com/tags/large-language-model",
            "https://datascience.stackexchange.com/tags/llm",
            "https://softwarerecs.stackexchange.com/tags/llm",
            "https://mathematica.stackexchange.com/tags/llm-functions",
            "https://or.stackexchange.com/tags/llm",
            "https://law.stackexchange.com/tags/llm",
            "https://hermeneutics.stackexchange.com/tags/llm"
          ],
          "P9100": "llm",
          "P691": "ph1274563"
        }
      }
    },
    {
      "name": "Low-rank approximation",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "LoRA technique used for parameter-efficient fine-tuning in the third progressive training stage.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Low-rank_approximation",
      "wikidata": {
        "id": "Q6692777",
        "label": "low-rank approximation",
        "description": "mathematical concept",
        "aliases": [],
        "claims": {
          "quora_topic_id": "Low-rank-Approximation",
          "mag_id": "90199385",
          "freebase_id": "/m/0hzs2zv",
          "openalex_id": "C90199385"
        }
      }
    },
    {
      "name": "Fine-tuning (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Training technique underlying instruction tuning and progressive training strategy for Uni-MoE.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
      "wikidata": {
        "id": "Q117286419",
        "label": "fine-tuning",
        "description": "process of taking a pre-trained model and further training it on a smaller, specific dataset to adapt or improve its performance for a particular task or domain",
        "aliases": [
          "neural network fine-tuning"
        ],
        "claims": {
          "instance_of": "Q117348143",
          "commons_category": "AI model fine-tuning",
          "follows": {
            "id": "Q124149651",
            "label": "pre-training"
          },
          "subclass_of": "Q1714153",
          "P10": "Easiest Way to Build Consistent Character - Step-by-Step Guide with OpenArt.webm"
        }
      }
    },
    {
      "name": "Data parallelism",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Modality-level data parallelism enables efficient distributed training across multimodal data.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Data_parallelism",
      "wikidata": {
        "id": "Q3124522",
        "label": "data parallelism",
        "description": "parallelization across multiple processors in parallel computing environments",
        "aliases": [],
        "claims": {
          "freebase_id": "/m/028b1y4",
          "mag_id": "61483411",
          "openalex_id": "C61483411",
          "subclass_of": {
            "id": "Q232661",
            "label": "parallel computing"
          }
        }
      }
    },
    {
      "name": "Attention (machine learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Fundamental mechanism in transformers underlying multimodal language model architectures.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
      "wikidata": {
        "id": "Q103701642",
        "label": "attention",
        "description": "machine learning technique",
        "aliases": [
          "attention mechanism"
        ],
        "claims": {
          "subclass_of": "Q6501338",
          "P1269": "Q2539",
          "google_kg_id": "/g/11qpclnpwr",
          "use": [
            "Q192776",
            {
              "id": "Q25325414",
              "label": "neural Turing machine"
            },
            {
              "id": "Q28324912",
              "label": "differentiable neural computer"
            },
            "Q85810444",
            "Q108281205"
          ],
          "P1557": "Q6501338",
          "openalex_topic_id": "413584",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/attention",
          "schematic": "Attention mechanism overview.svg"
        }
      }
    },
    {
      "name": "Transformer (machine learning model)",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "Base neural architecture for language models that Uni-MoE extends with MoE and multimodal capabilities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)",
      "wikidata": {
        "id": "Q85810444",
        "label": "transformer",
        "description": "machine-learning model architecture first developed by Google Brain",
        "aliases": [
          "transformer model",
          "transformer architecture",
          "transformers"
        ],
        "claims": {
          "subclass_of": [
            "Q192776",
            {
              "id": "Q113364611",
              "label": "deep learning model"
            }
          ],
          "developer": [
            {
              "id": "Q16927616",
              "label": "Google Brain"
            },
            {
              "id": "Q44749723",
              "label": "Ashish Vaswani"
            },
            "Q30251943"
          ],
          "follows": [
            {
              "id": "Q1457734",
              "label": "recurrent neural network"
            },
            {
              "id": "Q6673524",
              "label": "long short-term memory"
            }
          ],
          "described_by_source": {
            "id": "Q30249683",
            "label": "Attention Is All You Need"
          },
          "google_kg_id": "/g/11hz_m4ssw",
          "has_parts": [
            "Q42586063",
            {
              "id": "Q745243",
              "label": "decoder"
            }
          ],
          "uses": [
            {
              "id": "Q103701642",
              "label": "attention"
            },
            {
              "id": "Q5441227",
              "label": "feedforward neural network"
            }
          ],
          "use": [
            {
              "id": "Q30642",
              "label": "natural language processing"
            },
            {
              "id": "Q844240",
              "label": "computer vision"
            },
            {
              "id": "Q3245113",
              "label": "statistical machine translation"
            },
            {
              "id": "Q1394144",
              "label": "automatic summarization"
            }
          ],
          "schematic": "Transformer, full architecture.png",
          "stack_exchange_tag": "https://ai.stackexchange.com/tags/transformer",
          "P575": "2017-06-12",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Autoencoder",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Encoder-based architecture pattern relevant to modality-specific encoders in Uni-MoE.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Autoencoder",
      "wikidata": {
        "id": "Q786435",
        "label": "autoencoder",
        "description": "neural network that learns efficient data encoding in an unsupervised manner",
        "aliases": [
          "autoencoders",
          "AE"
        ],
        "claims": {
          "subclass_of": "Q192776",
          "quora_topic_id": "Autoencoder",
          "use": [
            "Q2493",
            "Q55564"
          ],
          "mag_id": "101738243",
          "different_from": {
            "id": "Q336945",
            "label": "Autocoder"
          },
          "P7502": "Autoencoder-349BGA",
          "freebase_id": "/m/0grsv6",
          "P9100": "autoencoder",
          "openalex_id": "C101738243",
          "openalex_topic_id": "207268",
          "P2888": "http://semantic-systems.net/swemls/StatisticalModel.AutoEncoder",
          "mesh_id": "D000098446",
          "mesh_tree_code": [
            "G17.035.250.500.250.250",
            "G17.035.250.500.750.500",
            "G17.035.297",
            "G17.485.500.250",
            "L01.224.050.375.530.250.250",
            "L01.224.050.375.530.750.500",
            "L01.224.050.375.605.500.250",
            "L01.224.050.395"
          ],
          "P1269": [
            "Q1152135",
            {
              "id": "Q197536",
              "label": "deep learning"
            }
          ],
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    },
    {
      "name": "Multimodal machine learning",
      "entity_type": "concept",
      "relation": "proposes",
      "relevance": "Uni-MoE advances multimodal machine learning by proposing the first unified MLLM with MoE architecture handling diverse modalities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multimodal_machine_learning",
      "wikidata": {
        "id": "Q25052564",
        "label": "multimodal learning",
        "description": "machine learning combining different information resources, such as images and text",
        "aliases": [],
        "claims": {
          "subclass_of": "Q2539",
          "mag_id": "2780660688",
          "freebase_id": "/m/013f3fbp",
          "openalex_id": "C2780660688",
          "commons_category": "Multimodal machine learning",
          "P3342": {
            "id": "Q97454550",
            "label": "Michal Valko"
          }
        }
      }
    }
  ]
}