{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Large language model",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "Llama 2 is built on large language model architecture and represents this technological foundation.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "wikidata": {
        "id": "Q115305900",
        "label": "large language model",
        "description": "language model built with very large amounts of texts",
        "aliases": [
          "LLM",
          "LLMs",
          "large language models",
          "word soup machine",
          "word soup model",
          "word salad machine",
          "word salad model"
        ],
        "claims": {
          "subclass_of": "Q3621696",
          "P1269": {
            "id": "Q11660",
            "label": "artificial intelligence"
          },
          "short_name": "LLM",
          "uses": [
            {
              "id": "Q117217619",
              "label": "AI prompt"
            },
            "Q85810444"
          ],
          "P5008": "Q117245199",
          "topic_main_category": {
            "id": "Q117280639",
            "label": "Category:Large language models"
          },
          "has_parts": [
            {
              "id": "Q116777014",
              "label": "generative pre-trained transformer"
            },
            "Q117246174",
            "Q3614994"
          ],
          "P973": "https://www.youtube.com/watch?v=WqYBx2gB6vA",
          "P5844": "neo-modello-linguistico-di-grandi-dimensioni_(Neologismi)",
          "opposite_of": "Q123759530",
          "google_kg_id": "/g/11kc9956b3",
          "gnd_id": "1322631905",
          "P12861": "E434",
          "use": [
            "Q116961403",
            {
              "id": "Q3510521",
              "label": "computer security"
            }
          ],
          "instance_of": {
            "id": "Q117349475",
            "label": "artificial intelligence model type"
          },
          "commons_category": "Large language models",
          "britannica_id": "topic/large-language-model",
          "P989": [
            "Wikipedia - Large language model (spoken by AI voice).mp3",
            "Wikipedia - Modelo extenso de lenguaje (hablado por voz AI).mp3"
          ],
          "mesh_id": "D000098342",
          "mesh_tree_code": [
            "G17.035.250.500.250.500",
            "G17.485.500.500",
            "L01.224.050.375.530.250.500",
            "L01.224.050.375.605.500.500",
            "L01.224.494"
          ],
          "P3911": "30478-6",
          "P2354": {
            "id": "Q131598147",
            "label": "list of large language models"
          },
          "P1368": "000355290",
          "described_by_source": "Q133280509",
          "P508": "77644",
          "P13572": "large+language+model",
          "stack_exchange_tag": [
            "https://genai.stackexchange.com/tags/llm",
            "https://ai.stackexchange.com/tags/large-language-models",
            "https://stats.stackexchange.com/tags/llm",
            "https://stackoverflow.com/tags/large-language-model",
            "https://datascience.stackexchange.com/tags/llm",
            "https://softwarerecs.stackexchange.com/tags/llm",
            "https://mathematica.stackexchange.com/tags/llm-functions",
            "https://or.stackexchange.com/tags/llm",
            "https://law.stackexchange.com/tags/llm",
            "https://hermeneutics.stackexchange.com/tags/llm"
          ],
          "P9100": "llm",
          "P691": "ph1274563"
        }
      }
    },
    {
      "name": "Fine-tuning (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Fine-tuning is the core technique used to adapt Llama 2 for dialogue and safety improvements.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
      "wikidata": {
        "id": "Q117286419",
        "label": "fine-tuning",
        "description": "process of taking a pre-trained model and further training it on a smaller, specific dataset to adapt or improve its performance for a particular task or domain",
        "aliases": [
          "neural network fine-tuning"
        ],
        "claims": {
          "instance_of": "Q117348143",
          "commons_category": "AI model fine-tuning",
          "follows": {
            "id": "Q124149651",
            "label": "pre-training"
          },
          "subclass_of": "Q1714153",
          "P10": "Easiest Way to Build Consistent Character - Step-by-Step Guide with OpenArt.webm"
        }
      }
    },
    {
      "name": "Dialogue system",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "The paper optimizes Llama 2-Chat specifically for dialogue conversation tasks.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Dialogue_system",
      "wikidata": {
        "id": "Q5270587",
        "label": "dialogue system",
        "description": "computer system intended to converse with a human",
        "aliases": [
          "dialog system",
          "conversational agent",
          "man-machine conversation"
        ],
        "claims": {
          "subclass_of": [
            "Q121182",
            {
              "id": "Q30642",
              "label": "natural language processing"
            }
          ],
          "mag_id": "190954187",
          "freebase_id": "/m/08y96v",
          "openalex_id": "C190954187",
          "P691": "ph351273",
          "google_kg_id": "/g/121hrdl8"
        }
      }
    },
    {
      "name": "Generative pre-trained transformer",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "Llama 2 is built on the transformer architecture which forms the basis of modern LLMs.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
      "wikidata": {
        "id": "Q116777014",
        "label": "generative pre-trained transformer",
        "description": "type of large language model",
        "aliases": [
          "GPT",
          "generative pretrained transformer"
        ],
        "claims": {
          "has_parts": [
            {
              "id": "Q116937684",
              "label": "GPT-J"
            },
            "Q115564437",
            {
              "id": "Q112702082",
              "label": "foundation model"
            },
            "Q116709136",
            {
              "id": "Q95726718",
              "label": "GPT-1"
            },
            "Q95726734",
            "Q95726727",
            {
              "id": "Q117791942",
              "label": "AutoGPT"
            },
            "Q116793893",
            {
              "id": "Q105078662",
              "label": "DALL-E"
            },
            "Q108582200",
            {
              "id": "Q118176939",
              "label": "Open Assistant"
            }
          ],
          "P5008": "Q117245199",
          "subclass_of": [
            {
              "id": "Q115305900",
              "label": "large language model"
            },
            "Q117246174",
            "Q85810444",
            "Q2539",
            {
              "id": "Q11660",
              "label": "artificial intelligence"
            }
          ],
          "different_from": "Q124237017",
          "short_name": "GPT",
          "google_kg_id": "/g/11ts8q7891",
          "P8313": "GPT",
          "P5019": "generative-pre-trained-transformer-informatik",
          "P11662": "3641756"
        }
      }
    },
    {
      "name": "Llama (language model)",
      "entity_type": "artifact",
      "relation": "proposes",
      "relevance": "The paper introduces Llama 2, the new generation of the Llama family of language models with improved safety and dialogue capabilities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Llama_(language_model)",
      "wikidata": {
        "id": "Q116894231",
        "label": "LLaMA",
        "description": "large language model by Meta AI",
        "aliases": [
          "Llama",
          "Large Language Model Meta AI"
        ],
        "claims": {
          "instance_of": [
            {
              "id": "Q115305900",
              "label": "large language model"
            },
            "Q7397",
            "Q124629760",
            "Q3621696",
            {
              "id": "Q431289",
              "label": "brand"
            }
          ],
          "P1324": [
            "https://github.com/facebookresearch/llama",
            "https://github.com/meta-llama/llama"
          ],
          "developer": "Q380",
          "P973": "https://research.facebook.com/file/1574548786327032/LLaMA--Open-and-Efficient-Foundation-Language-Models.pdf",
          "P5008": "Q117245199",
          "creator": {
            "id": "Q112114913",
            "label": "Meta AI"
          },
          "P577": "2023-02-00",
          "different_from": {
            "id": "Q112624345",
            "label": "LaMDA"
          },
          "P348": [
            "3",
            "2",
            "3.1",
            "3.2",
            "3.3",
            "4"
          ],
          "P8885": "LLaMA",
          "P856": "https://www.llama.com/",
          "P277": {
            "id": "Q28865",
            "label": "Python"
          },
          "P275": {
            "id": "Q123561541",
            "label": "Llama 2 Community License Agreement"
          },
          "P6216": {
            "id": "Q50423863",
            "label": "copyrighted"
          },
          "P3984": "LocalLLaMA",
          "has_parts": [
            {
              "id": "Q131938915",
              "label": "Llama 3.3"
            },
            {
              "id": "Q131938944",
              "label": "Llama 3.2"
            },
            {
              "id": "Q127726799",
              "label": "Llama 3.1"
            },
            "Q125522232",
            {
              "id": "Q120847029",
              "label": "Llama 2"
            },
            {
              "id": "Q120847097",
              "label": "Llama 1"
            },
            "Q137392462"
          ]
        }
      }
    }
  ]
}