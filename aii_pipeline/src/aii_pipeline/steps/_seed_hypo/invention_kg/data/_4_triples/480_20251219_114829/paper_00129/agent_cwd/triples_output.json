{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Large language model",
      "relation": "proposes",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "relevance": "Gemma is a new family of lightweight large language models that represents a novel contribution to open-source LLM development."
    },
    {
      "name": "Language model",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model",
      "relevance": "Language modeling is the foundational concept underlying Gemma's architecture and training approach."
    },
    {
      "name": "Natural language processing",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
      "relevance": "Gemma is designed to solve NLP tasks including language understanding and reasoning."
    },
    {
      "name": "Fine-tuning",
      "relation": "proposes",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
      "relevance": "The paper provides fine-tuned checkpoints of Gemma models, representing a novel application of fine-tuning techniques."
    },
    {
      "name": "Transfer learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transfer_learning",
      "relevance": "Gemma leverages transfer learning by building on research and technology from Gemini models."
    },
    {
      "name": "Generative pre-trained transformer",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer",
      "relevance": "Gemma likely uses transformer-based architecture similar to other modern language models in this category."
    },
    {
      "name": "Transformer",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)",
      "relevance": "The transformer architecture is the underlying neural network architecture for modern LLMs like Gemma."
    },
    {
      "name": "Attention mechanism",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)",
      "relevance": "Attention mechanisms are a core component of transformer-based language models like Gemma."
    },
    {
      "name": "Deep learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Deep_learning",
      "relevance": "Deep learning is the fundamental machine learning approach used to train Gemma models."
    },
    {
      "name": "Neural network",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
      "relevance": "Neural networks form the computational foundation of language models like Gemma."
    },
    {
      "name": "Document classification",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Document_classification",
      "relevance": "Document/text classification is one of the language understanding tasks Gemma is evaluated on."
    }
  ]
}
