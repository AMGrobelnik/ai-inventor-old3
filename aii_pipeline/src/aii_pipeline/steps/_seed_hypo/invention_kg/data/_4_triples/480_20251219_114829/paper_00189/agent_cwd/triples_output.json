{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Monte Carlo method",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "The paper proposes a Monte-Carlo simulation algorithm as the core technique for real-time policy improvement.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Monte_Carlo_method"
    },
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper builds on reinforcement learning principles for policy improvement using reward maximization.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
    },
    {
      "name": "Backgammon",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Backgammon is the primary application domain where the algorithm is experimentally evaluated.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Backgammon"
    },
    {
      "name": "Multilayer perceptron",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "The baseline TD-Gammon uses multilayer neural networks, which the Monte-Carlo algorithm improves upon.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multilayer_perceptron"
    },
    {
      "name": "TD-Gammon",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "TD-Gammon is a strong baseline neural network player that the proposed algorithm is compared against.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/TD-Gammon"
    },
    {
      "name": "Temporal difference learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Temporal difference learning is the underlying method used by TD-Gammon, serving as a comparison baseline.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Temporal_difference_learning"
    }
  ]
}
