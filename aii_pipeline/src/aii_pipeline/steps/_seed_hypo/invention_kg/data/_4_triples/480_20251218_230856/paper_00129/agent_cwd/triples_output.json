{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Large language model",
      "entity_type": "artifact",
      "relation": "proposes",
      "relevance": "Gemma is a new family of large language models that represent the core contribution of this paper.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model"
    },
    {
      "name": "Open-source software",
      "entity_type": "concept",
      "relation": "proposes",
      "relevance": "Gemma models are released as open-source with publicly available pretrained and fine-tuned checkpoints.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Open-source_software"
    },
    {
      "name": "Google Gemini",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "Gemma is built from research and technology used to create Gemini models.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Google_Gemini"
    },
    {
      "name": "Natural language understanding",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Language understanding is one of the core capabilities and benchmarks Gemma is evaluated on.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_understanding"
    },
    {
      "name": "Transformer (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Transformers are the fundamental architecture underlying Gemma models for language processing.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)"
    },
    {
      "name": "Fine-tuning (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Gemma provides both pretrained and fine-tuned checkpoints for various downstream tasks.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)"
    },
    {
      "name": "Transfer learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Transfer learning enables Gemma to leverage pre-training for improved performance on multiple tasks.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transfer_learning"
    },
    {
      "name": "AI safety",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Gemma includes comprehensive evaluations of safety and responsibility aspects as core contribution.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/AI_safety"
    },
    {
      "name": "Language model benchmark",
      "entity_type": "data",
      "relation": "uses",
      "relevance": "Gemma is evaluated across academic benchmarks including text-based tasks to measure performance.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model_benchmark"
    },
    {
      "name": "Natural language processing",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Language understanding and reasoning are core NLP tasks that Gemma addresses.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_processing"
    }
  ]
}
