{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Artificial neural network",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Artificial_neural_network",
      "relevance": "The paper studies how large neural networks learn linguistic structure through self-supervision."
    },
    {
      "name": "Self-supervised learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
      "relevance": "The paper demonstrates that models trained via self-supervision (masked word prediction) learn linguistic structure without explicit labels."
    },
    {
      "name": "Natural language processing",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_processing",
      "relevance": "The paper addresses how neural networks perform language understanding, a core NLP task."
    },
    {
      "name": "Parse tree",
      "relation": "proposes",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Parse_tree",
      "relevance": "The paper proposes methods to reconstruct sentence parse trees from learned embeddings, showing how models implicitly learn syntactic structure."
    },
    {
      "name": "Coreference",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Coreference",
      "relevance": "The paper demonstrates that neural network components focus on anaphoric coreference relationships between words."
    },
    {
      "name": "Word embedding",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Word_embedding",
      "relevance": "The paper analyzes learned word embeddings and shows how linear transformations capture parse tree distances."
    },
    {
      "name": "Syntax",
      "relation": "proposes",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Syntax",
      "relevance": "The paper proposes that neural networks emergently learn syntactic structure, revealing how grammatical relationships are encoded in model components."
    },
    {
      "name": "Language model",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model",
      "relevance": "The paper studies deep contextual language models trained on large amounts of text data."
    },
    {
      "name": "Deep learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Deep_learning",
      "relevance": "The paper studies how deep neural network architectures learn to encode linguistic knowledge."
    }
  ]
}
