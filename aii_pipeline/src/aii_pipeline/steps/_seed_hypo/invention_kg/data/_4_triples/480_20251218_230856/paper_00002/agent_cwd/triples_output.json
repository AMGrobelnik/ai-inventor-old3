{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Reinforcement learning",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "relevance": "Foundational machine learning paradigm that the paper builds upon for offline policy optimization."
    },
    {
      "name": "Offline learning",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Offline_learning",
      "relevance": "The core problem setting where policies are learned from fixed batch data without active exploration."
    },
    {
      "name": "Model-based reinforcement learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "relevance": "The primary approach used in MOPO that builds dynamic models to enable offline policy learning."
    },
    {
      "name": "Model-free reinforcement learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)",
      "relevance": "The baseline approach that MOPO outperforms and compares against in offline RL benchmarks."
    },
    {
      "name": "Markov decision process",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "relevance": "The mathematical framework used to formalize the offline reinforcement learning problem and theoretical analysis."
    },
    {
      "name": "Distributional domain adaptation",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Domain_adaptation",
      "relevance": "Addresses the distributional shift challenge between offline training data and policy-visited states."
    },
    {
      "name": "Uncertainty quantification",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Uncertainty_quantification",
      "relevance": "Core technique used in MOPO to penalize rewards based on dynamics uncertainty for safe offline learning."
    },
    {
      "name": "Robot control",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Robot_control",
      "relevance": "Application domain used to evaluate MOPO's performance on challenging continuous control tasks."
    },
    {
      "name": "Policy gradient",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Policy_gradient",
      "relevance": "Policy optimization method that forms the basis for the offline policy optimization approach."
    },
    {
      "name": "Model-based Offline Policy Optimization",
      "relation": "proposes",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "relevance": "Novel algorithm proposed that modifies model-based RL with uncertainty-penalized rewards for offline learning."
    }
  ]
}
