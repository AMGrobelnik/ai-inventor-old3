{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Core ML paradigm that offline RL and the MOPO algorithm are built upon",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
    },
    {
      "name": "Model-based reinforcement learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "The base approach that MOPO modifies and improves for offline settings",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
    },
    {
      "name": "Model-free reinforcement learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Contrasted approach that constrains policy to data support; MOPO outperforms these methods",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)"
    },
    {
      "name": "Markov decision process",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Theoretical framework used to formulate offline RL problem and analyze policy returns",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process"
    },
    {
      "name": "Policy optimization",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "The proposed MOPO algorithm directly optimizes policies with uncertainty-penalized rewards",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Proximal_policy_optimization"
    },
    {
      "name": "Uncertainty quantification",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "MOPO's key innovation: penalizing rewards by dynamics uncertainty to avoid distributional shift",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Uncertainty_quantification"
    }
  ]
}
