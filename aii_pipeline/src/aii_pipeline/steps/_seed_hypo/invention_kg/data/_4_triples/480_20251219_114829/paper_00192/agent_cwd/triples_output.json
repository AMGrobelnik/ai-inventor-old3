{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Partially observable Markov decision process",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper addresses gradient-based optimization directly on the policy performance in POMDPs, which is the core problem domain.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"
    },
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper applies reinforcement learning methodology to develop gradient-based policy optimization techniques.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
    },
    {
      "name": "Gradient descent",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Gradient ascent (the dual of gradient descent) is the core optimization technique underlying the GPOMDP algorithm.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Gradient_descent"
    },
    {
      "name": "Policy gradient method",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "GPOMDP is a novel REINFORCE-like algorithm that estimates policy gradients for average reward optimization in POMDPs.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Policy_gradient_method"
    },
    {
      "name": "Markov chain",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The paper explicitly uses a single sample path of the underlying Markov chain for gradient estimation.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_chain"
    },
    {
      "name": "Conjugate gradient method",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "The paper shows how gradient estimates from GPOMDP can be integrated into conjugate gradient procedures to find local optima.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Conjugate_gradient_method"
    }
  ]
}
