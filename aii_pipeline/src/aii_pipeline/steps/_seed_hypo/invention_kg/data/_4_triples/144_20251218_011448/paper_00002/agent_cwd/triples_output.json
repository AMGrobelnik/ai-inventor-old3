{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Reinforcement learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Core machine learning paradigm that MOPO builds upon for offline policy learning.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning"
    },
    {
      "name": "Policy gradient method",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Fundamental optimization approach for policy-based reinforcement learning algorithms.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Policy_gradient_method"
    },
    {
      "name": "Markov decision process",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Theoretical framework underlying the MDP formulation that MOPO optimizes a lower bound for.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process"
    },
    {
      "name": "Model-free (reinforcement learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Baseline approach that MOPO compares against; MOPO demonstrates advantages over model-free methods.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)"
    },
    {
      "name": "Domain adaptation",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Related concept addressing distributional shift, the core challenge MOPO tackles through uncertainty penalization.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Domain_adaptation"
    },
    {
      "name": "Control theory",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Foundational theory for continuous control tasks that MOPO evaluates on as part of its benchmarks.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Control_theory"
    },
    {
      "name": "Uncertainty quantification",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Key technique that MOPO employs to penalize model uncertainty and avoid distributional shift.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Uncertainty_quantification"
    },
    {
      "name": "Model predictive control",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Control approach that MOPO adapts for offline policy optimization using learned dynamics models.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Model_predictive_control"
    },
    {
      "name": "Offline learning",
      "entity_type": "concept",
      "relation": "proposes",
      "relevance": "The paper proposes MOPO, a novel model-based offline policy optimization method that improves offline learning in RL with uncertainty penalization.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Offline_learning"
    }
  ]
}
