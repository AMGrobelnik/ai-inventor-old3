{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "BERT (language model)",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "BERTurk adapts the pre-trained BERT model architecture for Turkish language processing.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/BERT_(language_model)"
    },
    {
      "name": "Turkish language",
      "entity_type": "task",
      "relation": "proposes",
      "relevance": "The paper introduces the first BERT models specifically trained for Turkish, addressing language-specific NLP challenges.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Turkish_language"
    },
    {
      "name": "Knowledge distillation",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "DistilBERTurk uses knowledge distillation to create a smaller, more efficient version of the base BERTurk model.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Knowledge_distillation"
    },
    {
      "name": "Hugging Face",
      "entity_type": "tool",
      "relation": "uses",
      "relevance": "The paper releases all BERTurk models on the Hugging Face model hub for public access and distribution.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Hugging_Face"
    }
  ]
}
