{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Neural network",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Neural_network_(machine_learning)",
      "relevance": "Large artificial neural networks are the primary systems studied for learning linguistic structure without explicit supervision."
    },
    {
      "name": "Self-supervised learning",
      "entity_type": "method",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Self-supervised_learning",
      "relevance": "The paper studies models trained via self-supervision using masked word prediction tasks."
    },
    {
      "name": "Natural-language understanding",
      "entity_type": "task",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural-language_understanding",
      "relevance": "Language understanding requiring hierarchical structure construction is the core challenge the paper addresses."
    },
    {
      "name": "Parse tree",
      "entity_type": "concept",
      "relation": "proposes",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Parse_tree",
      "relevance": "The paper proposes methods to approximately reconstruct parse tree structures from neural network embeddings."
    },
    {
      "name": "Word embedding",
      "entity_type": "concept",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Word_embedding",
      "relevance": "The paper analyzes learned embeddings in neural networks and shows linear transformations capture parse tree distances."
    },
    {
      "name": "Supervised learning",
      "entity_type": "method",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Supervised_learning",
      "relevance": "The paper contrasts self-supervised learning with traditional supervised learning on labeled treebanks."
    },
    {
      "name": "Deep learning",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Deep_learning",
      "relevance": "Deep contextual language models are the neural architectures studied for emergent linguistic structure."
    },
    {
      "name": "Language model",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model",
      "relevance": "Modern deep contextual language models are shown to learn syntactic and semantic structure."
    },
    {
      "name": "Anaphora (linguistics)",
      "entity_type": "concept",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Anaphora_(linguistics)",
      "relevance": "The paper demonstrates that neural network components focus on anaphoric coreference relationships."
    },
    {
      "name": "Coreference",
      "entity_type": "concept",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Coreference",
      "relevance": "The paper shows models learn to identify coreference relationships without explicit supervision."
    }
  ]
}
