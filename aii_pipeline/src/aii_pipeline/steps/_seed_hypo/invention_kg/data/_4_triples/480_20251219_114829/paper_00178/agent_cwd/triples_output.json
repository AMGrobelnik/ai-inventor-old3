{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Large language model",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "relevance": "The paper uses large language models as the foundation for implementing language agents."
    },
    {
      "name": "Reinforcement learning",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "relevance": "The paper proposes an alternative to traditional reinforcement learning methods by using verbal feedback instead of weight updates."
    },
    {
      "name": "Episodic memory",
      "relation": "proposes",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Episodic_memory",
      "relevance": "Reflexion agents maintain reflective text in an episodic memory buffer to improve decision-making across trials."
    },
    {
      "name": "GPT-4",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/GPT-4",
      "relevance": "GPT-4 is used as a baseline for comparison, achieving 80% on HumanEval compared to Reflexion's 91%."
    },
    {
      "name": "Sequential decision making",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Sequential_decision_making",
      "relevance": "Sequential decision-making is one of the key task domains where Reflexion demonstrates improvements."
    },
    {
      "name": "Code generation",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Code_generation",
      "relevance": "Code generation evaluated on HumanEval benchmark is a primary task domain where Reflexion achieves 91% accuracy."
    },
    {
      "name": "Automated reasoning",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Automated_reasoning",
      "relevance": "Language reasoning is one of the task domains used to evaluate Reflexion's performance."
    },
    {
      "name": "Benchmark",
      "relation": "uses",
      "entity_type": "tool",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Benchmark_(computing)",
      "relevance": "HumanEval benchmark is used to evaluate and compare Reflexion's code generation performance with other models."
    }
  ]
}
