{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Large language model",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "relevance": "Uni-MoE builds upon LLM architectures to create a unified multimodal system."
    },
    {
      "name": "Mixture of experts",
      "entity_type": "method",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Mixture_of_experts",
      "relevance": "The core architecture used to scale Uni-MoE efficiently with sparse expert routing."
    },
    {
      "name": "Fine-tuning (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)",
      "relevance": "LoRA-based fine-tuning is applied to adapt the model across multimodal instruction data."
    },
    {
      "name": "Multimodal learning",
      "entity_type": "concept",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multimodal_learning",
      "relevance": "The foundational concept for processing multiple data modalities in Uni-MoE."
    },
    {
      "name": "Transfer learning",
      "entity_type": "concept",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transfer_learning",
      "relevance": "Used in the progressive training strategy where knowledge is transferred across modalities."
    },
    {
      "name": "Unified multimodal large language model with mixture of experts",
      "entity_type": "artifact",
      "relation": "proposes",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Mixture_of_experts",
      "relevance": "Uni-MoE is the novel framework proposed that combines MoE with unified multimodal LLM capabilities."
    }
  ]
}
