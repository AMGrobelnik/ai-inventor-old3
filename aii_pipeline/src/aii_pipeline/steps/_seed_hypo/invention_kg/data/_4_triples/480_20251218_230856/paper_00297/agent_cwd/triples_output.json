{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "GPT-3",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "GPT-3 is the foundational pre-trained language model used as the backbone for the synthetic data generation algorithm.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/GPT-3"
    },
    {
      "name": "Automatic summarization",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Medical dialogue summarization is a specific application of automatic text summarization techniques to conversational medical data.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Automatic_summarization"
    },
    {
      "name": "Few-shot learning",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Few-shot learning (low-shot learning) is the key technique that enables scaling 210 examples to match the performance of 6400 labeled examples.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Few-shot_learning"
    },
    {
      "name": "Ensemble learning",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Ensemble methods are used to combine multiple models to improve the quality of synthetic data generation.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Ensemble_learning"
    },
    {
      "name": "Synthetic data",
      "entity_type": "data",
      "relation": "proposes",
      "relevance": "The paper proposes a novel algorithm to create synthetic training data using GPT-3 for medical dialogue summarization.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Synthetic_data"
    },
    {
      "name": "Natural language processing",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Medical dialogue summarization requires natural language processing techniques to extract medically relevant information from conversations.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_processing"
    }
  ]
}
