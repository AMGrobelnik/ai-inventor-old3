{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Mixture of experts",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Core architecture that Uni-MoE extends to multimodal settings for efficient conditional computation.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Mixture_of_experts"
    },
    {
      "name": "Large language model",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Foundation model type that Uni-MoE enhances with multimodal capabilities and MoE efficiency.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model"
    },
    {
      "name": "Low-rank approximation",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "LoRA technique used for parameter-efficient fine-tuning in the third progressive training stage.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Low-rank_approximation"
    },
    {
      "name": "Fine-tuning (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Training technique underlying instruction tuning and progressive training strategy for Uni-MoE.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)"
    },
    {
      "name": "Data parallelism",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Modality-level data parallelism enables efficient distributed training across multimodal data.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Data_parallelism"
    },
    {
      "name": "Attention (machine learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Fundamental mechanism in transformers underlying multimodal language model architectures.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)"
    },
    {
      "name": "Transformer (machine learning model)",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "Base neural architecture for language models that Uni-MoE extends with MoE and multimodal capabilities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)"
    },
    {
      "name": "Autoencoder",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Encoder-based architecture pattern relevant to modality-specific encoders in Uni-MoE.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Autoencoder"
    },
    {
      "name": "Multimodal machine learning",
      "entity_type": "concept",
      "relation": "proposes",
      "relevance": "Uni-MoE advances multimodal machine learning by proposing the first unified MLLM with MoE architecture handling diverse modalities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multimodal_machine_learning"
    }
  ]
}
