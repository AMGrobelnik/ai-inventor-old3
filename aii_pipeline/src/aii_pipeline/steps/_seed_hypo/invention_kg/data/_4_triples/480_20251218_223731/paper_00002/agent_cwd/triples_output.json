{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Model-Based Offline Policy Optimization",
      "relation": "proposes",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "relevance": "MOPO is the novel algorithm proposed in this paper that applies model-based RL to offline settings with uncertainty penalization to address distributional shift."
    },
    {
      "name": "Reinforcement Learning",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
      "relevance": "MOPO extends reinforcement learning techniques to the offline setting, building on core RL concepts and frameworks."
    },
    {
      "name": "Offline Learning",
      "relation": "uses",
      "entity_type": "task",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Offline_learning",
      "relevance": "The paper addresses the offline RL problem where learning occurs from a fixed batch of previously collected data without active exploration."
    },
    {
      "name": "Model-Free (reinforcement learning)",
      "relation": "uses",
      "entity_type": "method",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Model-free_(reinforcement_learning)",
      "relevance": "The paper compares MOPO against prior model-free offline RL methods, which constrain policies to the support of data."
    },
    {
      "name": "Markov Decision Process",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Markov_decision_process",
      "relevance": "MOPO provides theoretical guarantees by maximizing a lower bound of the policy's return under the true MDP framework."
    },
    {
      "name": "Domain Adaptation",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Domain_adaptation",
      "relevance": "Distributional shift between offline data and learned policy states is a key challenge that MOPO addresses through uncertainty penalization."
    }
  ]
}
