{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Large language model",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "relevance": "The paper uses LLMs as judges to evaluate chatbot assistants on open-ended questions."
    },
    {
      "name": "GPT-4",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/GPT-4",
      "relevance": "GPT-4 is used as the primary LLM judge in the study due to its strong performance and reasoning ability."
    },
    {
      "name": "Llama (language model)",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Llama_(language_model)",
      "relevance": "LLaMA variants are evaluated and benchmarked against other models to demonstrate the complementary nature of the proposed benchmarks."
    },
    {
      "name": "Vicuna LLM",
      "relation": "uses",
      "entity_type": "artifact",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Vicuna_LLM",
      "relevance": "Vicuna is another model evaluated in the study to demonstrate the utility of the proposed evaluation methodology."
    },
    {
      "name": "Benchmark (computing)",
      "relation": "uses",
      "entity_type": "concept",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Benchmark_(computing)",
      "relevance": "The paper builds on the concept of benchmarking to create new evaluation methods for LLM-based systems."
    },
    {
      "name": "Language model benchmark",
      "relation": "proposes",
      "entity_type": "data",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model_benchmark",
      "relevance": "The paper introduces MT-Bench as a novel language model benchmark for evaluating LLM-based chat assistants on multi-turn conversational tasks."
    },
    {
      "name": "Chatbot Arena",
      "relation": "proposes",
      "entity_type": "tool",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Chatbot_Arena",
      "relevance": "Chatbot Arena is a crowdsourced evaluation platform proposed for comparing and ranking different LLM-based chatbot models through human preferences."
    }
  ]
}
