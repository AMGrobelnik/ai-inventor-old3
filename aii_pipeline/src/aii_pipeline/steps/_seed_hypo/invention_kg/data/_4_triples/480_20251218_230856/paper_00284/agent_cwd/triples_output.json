{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "BERT (language model)",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "BERT is the foundational pre-trained transformer model that the paper extends with multimodal capabilities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/BERT_(language_model)"
    },
    {
      "name": "XLNet",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "XLNet is an autoregressive transformer model that the paper similarly extends for multimodal sentiment analysis.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/XLNet"
    },
    {
      "name": "Transformer (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Transformers are the underlying neural network architecture that powers both BERT and XLNet models.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)"
    },
    {
      "name": "Sentiment analysis",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "Sentiment analysis is the core NLP task addressed in the paper for multimodal data.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Sentiment_analysis"
    },
    {
      "name": "Multimodal sentiment analysis",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "The specific multimodal variant of sentiment analysis combining text, audio, and visual modalities.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Multimodal_sentiment_analysis"
    },
    {
      "name": "Fine-tuning (deep learning)",
      "entity_type": "method",
      "relation": "uses",
      "relevance": "Fine-tuning is the transfer learning technique used to adapt pre-trained models to downstream tasks.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)"
    },
    {
      "name": "Natural language processing",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "NLP is the broader field in which the paper's multimodal sentiment analysis work is situated.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_processing"
    },
    {
      "name": "Attention (machine learning)",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "The attention mechanism is a fundamental component of transformer architectures used in the paper's models.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)"
    },
    {
      "name": "Multimodal Adaptation Gate",
      "entity_type": "method",
      "relation": "proposes",
      "relevance": "MAG is the novel method proposed by the paper to enable BERT and XLNet to accept and process multimodal nonverbal data.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)"
    }
  ]
}
