{
  "paper_type": "contribution",
  "triples": [
    {
      "name": "Large language model",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Large_language_model",
      "relevance": "The paper uses LLMs as judges to evaluate other LLMs on open-ended questions."
    },
    {
      "name": "GPT-4",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/GPT-4",
      "relevance": "GPT-4 is used as a strong LLM judge to evaluate chat assistants and shown to match human preferences over 80%."
    },
    {
      "name": "Llama (language model)",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Llama_(language_model)",
      "relevance": "LLaMA model variants are evaluated as chat assistants in the paper's benchmarks."
    },
    {
      "name": "Vicuna LLM",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Vicuna_LLM",
      "relevance": "Vicuna is evaluated as a chat assistant in the paper's MT-Bench and Chatbot Arena evaluation."
    },
    {
      "name": "Chatbot",
      "entity_type": "artifact",
      "relation": "uses",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Chatbot",
      "relevance": "The paper evaluates LLM-based chat assistants and their performance using LLMs as judges."
    },
    {
      "name": "Language model benchmark",
      "entity_type": "data",
      "relation": "proposes",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Language_model_benchmark",
      "relevance": "MT-Bench is a proposed multi-turn question benchmark for evaluating LLM-based chat assistants."
    },
    {
      "name": "Crowdsourcing",
      "entity_type": "method",
      "relation": "proposes",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Crowdsourcing",
      "relevance": "Chatbot Arena uses crowdsourced battle platform methodology with 30K crowdsourced conversations to validate LLM judges."
    },
    {
      "name": "Benchmark (computing)",
      "entity_type": "concept",
      "relation": "proposes",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Benchmark_(computing)",
      "relevance": "The paper proposes MT-Bench, a new benchmark for evaluating LLM-based chat assistants."
    }
  ]
}
