{
  "paper_type": "survey",
  "triples": [
    {
      "name": "Transformer (deep learning)",
      "entity_type": "artifact",
      "relation": "uses",
      "relevance": "Transformers are the primary subject of this survey on position encoding methods.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Transformer_(deep_learning)"
    },
    {
      "name": "Natural language processing",
      "entity_type": "task",
      "relation": "uses",
      "relevance": "The paper discusses Transformers in the context of NLP research applications.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Natural_language_processing"
    },
    {
      "name": "Word order",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Word order is highlighted as essential to the semantics and syntax that position encoding must preserve.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Word_order"
    },
    {
      "name": "Attention (machine learning)",
      "entity_type": "concept",
      "relation": "uses",
      "relevance": "Attention is the core mechanism underlying Transformers and interacts with position information.",
      "wikipedia_url": "https://en.wikipedia.org/wiki/Attention_(machine_learning)"
    }
  ]
}
